---
title: "R Data Science Course 9: Choose Your Own Project"
author: "James Lo"
date: '2022-06-09'
output: 
  pdf_document:
    latex_engine: xelatex
    df_print: kable
    toc : TRUE
    toc_depth: 3
urlcolor: blue
mainfont: Calibri Light
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Introduction

(words go here)

## Data Loading

```{r packages, warning = F}
if(!require(tidyverse)) install.packages("tidyverse",
                                         repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret",
                                     repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", 
                                          repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", 
                                     repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", 
                                      repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", 
                                         repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", 
                                       repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", 
                                       repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", 
                                       repos = "http://cran.us.r-project.org")
if(!require(viridis)) install.packages("viridis", 
                                       repos = "http://cran.us.r-project.org")
if(!require(MLmetrics)) install.packages("MLmetrics", 
                                       repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", 
                                       repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
library(dslabs)
library(dplyr)
library(lubridate)
library(ggplot2)
library(readr)
library(readxl)
library(viridis)
library(MLmetrics)
library(rpart.plot)
# will add more packages as needed

```

```{r data_download, warning = FALSE}
uci_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip'

download.file(uci_url, 'DryBeanDataset.zip')
```
```{r data_unzip, warning = FALSE}
unzip('DryBeanDataset.zip')
```

```{r data_read, warning = FALSE}
# specify path
path <- paste0(getwd(), '/DryBeanDataset/Dry_Bean_Dataset.xlsx')
# read the first sheet (where the data is)
beans <- read_xlsx(path, sheet = 1)
```

## Exploratory Analysis

### Overview

```{r data_overview}
# overview of the data's size and format
str(beans)
```
The dataset contains 13611 entries, each representing a scanned bean. There are 12 metrics describing the dimensions of each bean, plus four 'shape factors' which are derived from a previous machine learning article on rice grains (cite).

### Checking for NA Values

```{r na_check}
which(is.na(beans))
```
As described on the UCI website, the dataset indeed contains no NA entries.

### Distribution of Outcome Classes

```{r num_classes}
# checking the distribution of classes (bean types) in the dataset
beans %>% group_by(Class) %>%
  summarize(count = n())
```
The dataset has a rather uneven distribution of the 7 bean varieties. The most frequently found type is Dermason at 3546 entries, while the least common is Bombay at 522 entries. This uneven distribution must be accounted for during model fitting and accuracy assessments.

### Distribution of Potential Predictor Variables Among Classes

Before models could be built, the distribution of values among differnt bean varieties were visualized with boxplots for all 16 possible predictor variables. This was done to understand the levels of variability between bean varieties that were present in each potential predictor, and select the most informative predictors for models that perform best with lower-dimensional data.

#### Area

```{r Area_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Area > (quantile(Area, 0.75)) + 1.5 * IQR(Area) | 
           Area < (quantile(Area, 0.25)) - 1.5 * IQR(Area))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Area), y = Area)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Areas')
```
The area IQR for Bombay was distinctively higher than those of all other varieties. Heavily overlapping IQRs exist in Cali and Barbunya, as well as Sira and Seker. Dermason, Seker, and Sira all have much narrower IQRs compared to the other varieties, despite having very close distribution ranges. Most outliers were found on the upper ends of their respective distributions.

#### Perimeter

```{r Perimeter_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Perimeter > (quantile(Perimeter, 0.75)) + 1.5 * IQR(Perimeter) | 
           Perimeter < (quantile(Perimeter, 0.25)) - 1.5 * IQR(Perimeter))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Perimeter), y = Perimeter)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Perimeters')
```
Similarly to the Area metric, Bombay had a distinctively higher range of Perimeter lengths than the other varieties. The IQR for Cali existed entirely within that of Barbunya, while the IQRs for Dermason and Seker overlapped. The IQRs of perimeter lengths for smaller bean varieties were less tight than seen in the Area graph, but also somewhat more distinct relative to the scale of the y-axis.

#### Major Axis Length

```{r MajorAxisLength_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(MajorAxisLength > (quantile(MajorAxisLength, 0.75)) + 1.5 * IQR(MajorAxisLength) | 
           MajorAxisLength < (quantile(MajorAxisLength, 0.25)) - 1.5 * IQR(MajorAxisLength))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, MajorAxisLength), y = MajorAxisLength)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Major Axis Lengths')
```
For Major Axis Length, Bombay once again had an IQR that was distinctively larger than all others. Barbunya/Horoz and Dermason/Seker had heavily overlapping IQRs, while Cali had a smaller IQR overlap with Horoz/Barbunya. Seker and Sira had more outliers on the high end of their ranges, while Horoz had more on the low end.

#### Minor Axis Length

```{r MinorAxisLength_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(MinorAxisLength > (quantile(MinorAxisLength, 0.75)) + 1.5 * IQR(MinorAxisLength) | 
           MinorAxisLength < (quantile(MinorAxisLength, 0.25)) - 1.5 * IQR(MinorAxisLength))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, MinorAxisLength), y = MinorAxisLength)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Minor Axis Lengths')
```

The distribution of Minor Axis Lengths was similar to that of Major Axis lengths, but with a few differences: here the heaviest overlap was between Cali and Barbunya, followed by Horoz/Sira and Sira/Seker. Also, Dermason had a partial IQR overlap with Horoz rather than a near-complete one with Seker. Most outliers were on the high ends of their respective groups. Cali, Seker, and Horoz had more outliers on th high ends of their respective groups, while Sira had more on the low end.

#### Aspect Ratio

```{r AspectRation_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(AspectRation > (quantile(AspectRation, 0.75)) + 1.5 * IQR(AspectRation) | 
           AspectRation < (quantile(AspectRation, 0.25)) - 1.5 * IQR(AspectRation))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, AspectRation), y =AspectRation)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Aspect Ratios')
```

The Aspect Ratio distributions differed from the previous potential predictors in that Bombay no longer had the most distinct distribution of values. Instead, Horoz and Cali had IQRs distinctively higher than others, while Seker had a lower IQR. The remaining varieties (Dermason, Barbunya, Sira, Bombay) all had heavily overlapping IQRs. Horoz had more outliers on the bottom end of its distribution, while Sira, Dermason, and Seker had more on their high ends.

#### Eccentricity

```{r Eccentricity_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Eccentricity > (quantile(Eccentricity, 0.75)) + 1.5 * IQR(Eccentricity) | 
           Eccentricity < (quantile(Eccentricity, 0.25)) - 1.5 * IQR(Eccentricity))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Eccentricity), y = Eccentricity)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Eccentricities')
```

The Eccentricity distributions were similar to that of Aspect Ratios, but with the two varieties having higher IQRs (Horoz, Cali) deviating from the others less while the one with lower IQR (Seker) deviating more. Most outliers were on the low ends of their respective ranges.

#### Convex Area

```{r ConvexArea_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ConvexArea > (quantile(ConvexArea, 0.75)) + 1.5 * IQR(ConvexArea) | 
           ConvexArea < (quantile(ConvexArea, 0.25)) - 1.5 * IQR(ConvexArea))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ConvexArea), y = ConvexArea)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Convex Areas')

```

With the Covex Area Sizes, a distribution somewhat similar to the Area, Perimeter, and Axis Length distributions could be seen again. Bombay had a distinctively high IQR of Convex Areas, while Dermason had the lowest range. Cali and Barbunya had heavily overlapping IQRs, with less overlap existing among the other varieties. The narrowest IQRs belonged to Seker and Sira. Outliers were biased towards the top end for Bombay, Cali, Barbunya, Sira, and Seker.

#### Equivalent Diameter

```{r EquivDiameter_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(EquivDiameter > (quantile(EquivDiameter, 0.75)) + 1.5 * IQR(EquivDiameter) | 
           EquivDiameter < (quantile(EquivDiameter, 0.25)) - 1.5 * IQR(EquivDiameter))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, EquivDiameter), y = EquivDiameter)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Equivalent Diameters')
```

The distributions of Equivalent Diameters were fairly similar to that of Convex Areas. Once again, Cali and Barbunya had heavy overlaps, with Bombay having a distinctly higher IQR. Cali and Seker had more outliers on the top ends of their distributions, while Horoz had more on the bottom end.

#### Extent

```{r Extent_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Extent > (quantile(Extent, 0.75)) + 1.5 * IQR(Extent) | 
           Extent < (quantile(Extent, 0.25)) - 1.5 * IQR(Extent))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Extent), y = Extent)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Extents')
```

The distribution of Extent Ratios exhibited heavy IQR overlaps between all bean varieties. Among the three groups with outliers (Cali, Seker, Bombay), all had more on the bottom ends of their respective distributions.

#### Solidity

```{r Solidity_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Solidity > (quantile(Solidity, 0.75)) + 1.5 * IQR(Solidity) | 
           Solidity < (quantile(Solidity, 0.25)) - 1.5 * IQR(Solidity))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Solidity), y = Solidity)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Solidities')
```

The distribution of Solidity values exhibited heavy IQR overlaps between nearly all bean varieties. Also, every bean variety had many outliers (values outside the boxplot whiskers) on the lower end of their respective distributions. Outliers were abundant, and located on the low ends of every group's distribution.

#### Roundness

```{r roundness_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(roundness > (quantile(roundness, 0.75)) + 1.5 * IQR(roundness) | 
           roundness < (quantile(roundness, 0.25)) - 1.5 * IQR(roundness))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, roundness), y = roundness)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Roundnesses')
```

The ranking of median Roundness values were similar to that of that of Solidity, but with the lowest three (Cali, Barnunya, Horoz) switched. Less overlap existed than in Solidity (except for heavy overlap Horoz and Barbunya), but many outliers outside the whiskers existed for all varieties. Outliers were abundant, and biased towards the low end of the range for all groups except Bombay.

#### Compactness

```{r Compactness_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Compactness > (quantile(Compactness, 0.75)) + 1.5 * IQR(Compactness) | 
           Compactness < (quantile(Compactness, 0.25)) - 1.5 * IQR(Compactness))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Compactness), y = Compactness)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Compactness Values')
```

The distribution of Compactness values had heavy overlaps between Bombay, Sira, Barbunya, and Dermason. Seker had the highest range of Compactness, while Horoz had the lowest. All varieties had many outliers, mostly on the high ends of the distributions. Seker had most of its outliers on the low end of its range; the opposite was true for Bombay, Cali, and Horoz.

#### Shape Factor 1

```{r ShapeFactor1_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor1 > (quantile(ShapeFactor1, 0.75)) + 1.5 * IQR(ShapeFactor1) | 
           ShapeFactor1 < (quantile(ShapeFactor1, 0.25)) - 1.5 * IQR(ShapeFactor1))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor1), y = ShapeFactor1)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 1')
```

The distributions of Shape Factor 1 values featured heavy overlap between Barbunya/Cali, and relatively less overlap between Seker/Sira/Horoz. Dermason had the highest range of Shape Factor 1, and Bombay had the lowest. All groups had more outliers on the top ends of their ranges except for Seker (more on the low end) and Bombay (two outliers on the low end only).

#### Shape Factor 2

```{r ShapeFactor2_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor2 > (quantile(ShapeFactor2, 0.75)) + 1.5 * IQR(ShapeFactor2) | 
           ShapeFactor2 < (quantile(ShapeFactor2, 0.25)) - 1.5 * IQR(ShapeFactor2))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor2), y = ShapeFactor2)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 2')
```

The distributions of Shape Factor 2 values had relatively less IQR overlap compared to most other variables examined, with the 4 highest Shape Factor 2 ranges (Seker, Dermason, Sira, Barbunya) all having non-overlapping IRRs. Horoz and Cali had a large IQR overlap, but neither had overlapping IQRs with Bombay. However, outliers on the high ends of the ranges were common. With the exception of Seker, all groups had more outliers on the top ends of their ranges.

#### Shape Factor 3

```{r ShapeFactor3_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor3 > (quantile(ShapeFactor3, 0.75)) + 1.5 * IQR(ShapeFactor3) | 
           ShapeFactor3 < (quantile(ShapeFactor3, 0.25)) - 1.5 * IQR(ShapeFactor3))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor3), y = ShapeFactor3)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 3')
```

The distributions of Shape Factor 3 values had heavy overlaps in Bombay, Sira, Barbunya, and Dermason. Seker had a distinctively higher IQR than the other varieties but also many low outliers; the opposite was true for Horoz. Except for Seker, all groups had more outliers on the low ends of their ranges.

#### Shape Factor 4

```{r ShapeFactor4_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor4 > (quantile(ShapeFactor4, 0.75)) + 1.5 * IQR(ShapeFactor4) | 
           ShapeFactor4 < (quantile(ShapeFactor4, 0.25)) - 1.5 * IQR(ShapeFactor4))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor4), y = ShapeFactor4)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 4')
```

The distributions for Shape Factor 4 exhibited heavy overlaps between nearly all varieties, with the possible exception of Seker. In addition, every variety had many outliers on the low ends of their respective distributions. All groups had outliers on the low ends of their distributions; Horoz had an exceptionally long range of outlier values.

### Combining Predictors for Scatterplots

Some common classification models, such as KNN, do not perform efficiently with high-dimentional data. As such, it would be constructive to select a pair of "compatible" predictor variables for those models. The goal is to have each variable contain inter-group (between bean varieties) variances that the other does not have sufficient amounts of. Also, the variables should be independent, lacking in redundancy of measurements. For instance, if one variable in the pair is already derived from bean lengths, the other variable should not also contain bean lengths in its formula. In addition, each of the two variables should have as much distance (or failing that, the least amount of overlap) between its group IQRs as possible, with the least amount (and skew) of outliers. Note that while some variables (like Area and Convex Area) exist on a far larger scale (at least in numbers alone) than others, normalization would make relative distances along the y-axis (as seen on the boxplots) more relevant than raw numerical differences.

To identify promising combinations, the variables were roughly grouped into six categories, reflecting broad features in their boxplots:

* Group 0 ("don't bother"): Variables whose boxplots displayed heavily overlapping IQRs and outliers that are visibly more numerous (compared to the other variables in this dataset) and/or heavily skewed in distributions. Extent, Solidity, and Shape Factor 4 were placed under this category.

* Group 1 ("Bombay high, Dermason low"): Variables where Bombay had the highest IQR (usually by a relatively large margin), and Dermason the lowest. Other bean varieties tended to have less separate or even overlapping IQRs. Area, Perimeter, Major Axis, Minor Axis, Convex Area, and Equivalent Diameter all fell under this category.

* Group 2 ("Horoz high, Seker low"): Variables where Horoz had the highest IQR, and Seker the lowest. Again, other groups tended to have less separate or overlapping IQRs. Aspect Ratio and Eccentricity fell under this cetegory.

* Group 3 ("Seker high, Horoz low"): The opposite of group 2.  Roundness and Compactness fell under this category. Note that as described in the original paper, Roundness and Compactness are both measures of how round a bean is, albeit calculated by different formulae. Both are also, by definition, the opposite metric of Aspect Ratio (which measures how elongated a bean is through the ratio between length/Major Axis Length and width/Minor Axis Width).

* Group 4 ("Dermason high, Bombay low"): The opposite of group 1, at least in terms of the defining feature (arbitrarily assigned for quick grouping). Shape Factor 1 was the sole variable assigned to this category.

* Group 5 ("Seker high, Bombay low"): Shape Factor 2 was the only variable assigned under this group.

Among group 1 variables, Perimeter and Equivalent Diameter appeared to be the most suitable choices, as they both had relatively less overlaps between the "middle groups" representing varieties other than Bombay and Dermason. Perimeter had large overlaps in Cali/Barbunya and Seker/Dermason, while Equivalent Diameter had large overlaps in Cali/Barbunya and Sira/Seker. Observing the boxplots, Aspect Ratio, Compactness, Shape Factor 2, and Shape Factor 3 were selected as the best complements due to having higher variances lacking in the group 1 choices, in addition to relatively less numerous and skewed outliers. Equivalent Diameter is incompatible with Compactness (Equivalent Diameter divided by Major Axis length), Shape Factor 2 (Minimum Axis length divided by Area), and Shape Factor 3 (which is derived from Area and Major Axis length). Meanwhile, Perimeter is compatible with Aspect Ratio, Compactness, Shape Factor 2, and Shape Factor 3. Included below are five scatterplots to further visualize the variables' ability to model bean varieties as distinct clusters of data points.

```{r trim_group0}
# trim off group 0 ("don't bother") variables, which will not be considered further
beans <-
  subset(beans, select = -c(Extent, Solidity, ShapeFactor4))
head(beans, 3)
```

```{r Perim_AR}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         ar_tf = (AspectRation - min(AspectRation)) / (max(AspectRation) - min(AspectRation))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = ar_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Perimeter and Aspect Ratio Combinations on a Cartesian Plane')
```

```{r Perim_Comp}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         comp_tf = (Compactness - min(Compactness)) / (max(Compactness) - min(Compactness))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = comp_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Compactness (Normalized)') +
  ggtitle('Normalized Perimeter and Compactness Combinations on a Cartesian Plane')
```

```{r Perim_SF2}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         sf2_tf = (ShapeFactor2 - min(ShapeFactor2)) / (max(ShapeFactor2) - min(ShapeFactor2))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = sf2_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Shape Factor 2 (Normalized)') +
  ggtitle('Normalized Perimeter and Shape Factor 2 Combinations on a Cartesian Plane')
```


```{r Perim_SF3}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         sf3_tf = (ShapeFactor3 - min(ShapeFactor3)) / (max(ShapeFactor3) - min(ShapeFactor3))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = sf3_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Shape Factor 3 (Normalized)') +
  ggtitle('Normalized Perimeter and Shape Factor 3 Combinations on a Cartesian Plane')
```

```{r ED_AR}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(ed_tf = (EquivDiameter - min(EquivDiameter)) / (max(EquivDiameter) - min(EquivDiameter)),
         ar_tf = (AspectRation - min(AspectRation)) / (max(AspectRation) - min(AspectRation))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = ed_tf, y = ar_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Equivalent Diameter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Equivalent Diameter and Aspect Ratio Combinations on a Cartesian Plane')
```

In all 5 scatterplots, adding another variable improved separation of the bean Class clusters that were poorly resolved in the group 1 variables (Barbunya/Cali, Sira/Seker, Seker/Dermason). However, large overlaps could still observed, especially in Dermason/Sira and Barbunya/Cali. Although the separation between Barbunya and Cali had improved from the near complete overlap in Perimeter and Equivalent Diameter boxplots, the scatterplots still revealed that a sizeable portion of the Barbunya cluster had been covered by the Cali cluster. Cluster visualization would be useful for further determination of which variable combination had the least amounts of overlap, but such a procedure would involve visualizing already tuned clustering algorithms and defeat the purpose of exploratory analysis. As a result, all five variable combinations were chosen for model building.

# Methods

## Data Formatting

Split into predictors (x; everything but Class) and outcomes (y; Class).

```{r x_y_split}
beans_x <- subset(beans, select = -c(Class))
beans_y <- subset(beans, select = c(Class))
head(beans_y, 3)
```

Turn predictors (known to be all numerical) into matrix for easier manipulation.

```{r x_as_matrix}
beans_x <- as.matrix(beans_x)
head(beans_x, 3)
```

As previously done in the scatterplots, the predictor variables must be normalized for algorithms such as KNN to work effectively (incluide explanation here). For an intuitive scale, all numeric perdictors were nromalized on a scale of 0 (minimum value) to 1 (maximum value) (insert formula here...)

```{r x_normalization}
# for every column, apply the range formula onto each cell value:
# new value = (value - column min) / (column max - column min)
max_predictors <- apply(beans_x, 2, max) # column maxima
min_predictors <- apply(beans_x, 2, min) # column minima
colranges <- max_predictors - min_predictors # column ranges
beans_x <- sweep(beans_x, 2, STAT = min_predictors, FUN = '-') # subtract minima
beans_x <- sweep(beans_x, 2, STAT = colranges, FUN = '/') # divide by col ranges
# x is each matrix cell value, y is each entry in colranges
head(beans_x, 3)
```
```{r testing_matrix_norm_ED_AR}
# reconstitute the data table
as.data.frame(beans_x) %>%
  mutate(Class = beans_y$Class) %>%
 # plot with coloring by bean variety (Class)
  ggplot(aes(x = EquivDiameter, y = AspectRation, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Equivalent Diameter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Equivalent Diameter and Aspect Ratio Combinations on a Cartesian Plane')
```
All right, normalization seems legit...

## Training and Validation
Consider a 80-20 split for train/val

Explain that 1) it's a common split and 2) one source listed it as a good ratio for your data size

Note that in most online sources outside EdX, the comparison dataset used to gauge final model performance is called the "training set", and the comparison dataset used for tuning is the "vaidation set"; in short, the two terms are switched between EdX and outside sources. For the sake of consistency, this project will use the EdX naming conventions.

```{r data_split}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# creating the validation indeces
val_index <-
  createDataPartition(y = beans$Class,
                      times = 1,
                      p = 0.2,
                      list = F)
# split the data
# as the x-y split had been made earlier, the train-val split must be made for both x and y
beans_x_train <- beans_x[-val_index,]
beans_y_train <- beans_y[-val_index,]
beans_x_val <- beans_x[val_index,]
beans_y_val <- beans_y[val_index,]
```
Unlike with the previous project (MovieLens Rating Prediction), the models used in this project should not reply on the same titles being present in the training and validation (or testing) sets. As such, the `semi_join` function will not be used to ensure that everything in the validation set being present in the training set. However, we must still make sure that both sets have all 7 bean classes.

```{r val_set_dim}
dim(beans_x_val)
```
```{r val_set_classes}
as.data.frame(beans_y_val) %>%
  group_by(Class) %>%
  summarize(counts = n())
```

```{r train_set_rows}
dim(beans_x_train)
```
```{r train_set_classes}
as.data.frame(beans_y_train) %>%
  group_by(Class) %>%
  summarize(counts = n())
```
The dimensions (specifically the number of rows) add up, and both sets have all 7 bean classes.

To facilitate the use of algorithms under caret's train function, the predictors and outcomes will be split


## Accuracy Metric
You are running a classification model, so it's either accuracy, confusion matrix, or F1 score (I guess...)
-F1 chosen over accuracy due to imbalance in data (despite the original paper's claims of even class dist.)

-Tune with multiclass F1, then examine tuned results more closely with multi-class confusion matrix
-> will need to do a custom function under trainControl
-> or set metric = F and set summary function = pr summary under traincontrol?
-> address issue of precision and recall: in the context of classifying beans, unless if large differences in value exist between bean varieties, false positives vs false negatives should not make too much of a difference

(read paper, how to calc F1, and setting custom performance metric funcs in caret later)

```{r custom_multiclass_f1}
# this func should take same args as a function called defaultSummary()
# data = a dataframe with columns 'obs' and 'pred' (observed/actual and predicted)
macro_f1 <- function(data, lev = NULL, model = NULL) {
  mF1 <- mean(f1(data$pred, data$obs)) # arithmetic mean for macro F1
  names(mF1) <- 'mF1'# names the function output
  return(mF1)
}

f1 <- function(predicted, actual) {
  mat <- as.matrix(table(predicted, actual)) # should make a table where rows are pred, cols are actual/obs
  precision <- diag(mat) / rowSums(mat) # true pos / all predicted pos
  recall <- diag(mat) / colSums(mat) # true pos / all actual pos
  f1 <- ifelse(precision + recall == 0,
                      0,
                      2 * (precision * recall) / (precision + recall))
  return(f1)
}

```

```{r test_matrix}
test_table <- data.table(obs = c(1, 1, 2, 2, 2, 3, 4),
           pred = c(1, 1, 2, 2, 1, 3, 4))

test_table
```

```{r table_test}
as.matrix(table(test_table$pred, test_table$obs))
```
```{r f1_test}
f1(test_table$pred, test_table$obs) # okay so the f1 function returns something... and the right answers to boot!
```
```{r macro_f1_test}
macro_f1(test_table) # the macro_f1 func as a whole works
```

## Cross-Validation
Consider 10-fold to minimize variance in accuracy metric... Holdout not used, as setting aside enough test data would shrink the already reduced train set too much. Note that with 10-fold, each fold would be 8 % (10 % of 80 %) of the full beans dataset

```{r xval_method}
# 10-fold cross-validation
tenfold_xval <-
  trainControl(method = 'cv',
               number = 10,
               p = 0.9,
               summaryFunction = macro_f1) # custom macro f1 func in progress
```

## Model Choices

-models for categorical classification (KNN, SVD, kmeans DT, RF?) -> might just end up with all 5 models being tuned for the first three
-> you don't have all year; just do log. reg for model 1 and one of the above for model 2, then maybe one more if you realy want.

-problem types: multiclass classification (assigning instances into three or more classes), supervised (dataset already labeled with classes)
-kmeans is unsupervised, so probably not using that
-KNN sounds ideal for your purposes

-three models: log reg, KNN, SVD (last one is like a bonus) -> finish first two quick so that you have time to do the third
-SVD is unsupervised... Maybe use SVM?
-Or have Decision Trees/Random Forests be the third model?
-If tree-based, will need multiple (non-redundant) predictors (the 5 categories would still help with variable combination; will exclude things from group 0 when choosing)

-log reg, KNN, DT, RF (4 models) then...

-For DT, RF: use a mix of 3 predictors (Perim, EQ, one more out of AR/Comp/SF3/SF2, depending on which one performed best in KNN)

# Results

## Model 1: Logistic Regression

```{r model1_Perimeter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,5)], # col 2 for perimeter, 5 for aspect ratio
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1a$results
```

```{r model1_Perimeter_Compactness}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,10)], # col 2 for perimeter, 10 for compactness
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1b$results
```
```{r model1_Perimeter_ShapeFactor2}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1c$results
```
```{r model1_Perimeter_ShapeFactor3}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1d <-
  train(y = beans_y_train$Class,
                x =  beans_x_train[,c(2,13)], # col 2 for perimeter, 13 for SF3
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1d$results
```
```{r model1_EquivDiameter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1e <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(8,5)], # col 8 for equiv. diam., 5 for aspect ratio
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1e$results
```

## Model 2: K-Nearest Neighbors (KNN)

```{r model2_Perimeter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,5)], # col 2 for perimeter, 5 for aspect ratio
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

# Warning in train.default(y = beans_y_train$Class, x = beans_x_train[, c(2,  :
#   The metric "mF1" was not in the result set.  will be used instead.
# -> using the names() func to name the output object of macro_f1 did the trick
ggplot(model_2a, highlight = T)
```
```{r model_2a_besttune}
model_2a$bestTune # k = 90 is best for this one
model_2a$results[90,]
```

```{r model2_Perimeter_Compactness}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,10)], # col 2 for perimeter, 10 for compactness
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2b, highlight = T)
```

```{r model_2b_besttune}
model_2b$bestTune # k=86
model_2b$results[86,]
```


```{r model2_Perimeter_ShapeFactor2}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2c, highlight = T)
```

```{r model_2c_besttune}
model_2c$bestTune # k = 49
model_2c$results[49,]
```

```{r model2_Perimeter_ShapeFactor3}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2d <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,13)], # col 2 for perimeter, 13 for SF3
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2d, highlight = T)
```

```{r model_2d_besttune}
model_2d$bestTune # k=79
model_2d$results[79,]
```

```{r model2_EquivDiameter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2e <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(8,5)], # col 8 for equiv. diam., 5 for aspect ratio
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2e, highlight = T)
```

```{r model_2e_besttune}
model_2e$bestTune # k=74
model_2e$results[74,]
```

## Comparing Two-Variable Model Results

```{r models_1_2_table}
models_1_2 <-
  data.table(variables = c('Perimeter, Aspect Ratio',
                           'Perimeter, Compactness',
                           'Perimeter, Shape Factor 2',
                           'Perimeter, Shape Factor 3',
                           'Equivalent Diameter, Aspect Ratio'),
             logreg_mF1_unregularized = c(model_1a$results[,2],
                            model_1b$results[,2],
                            model_1c$results[,2],
                            model_1d$results[,2],
                            model_1e$results[,2]),
             knn_mF1 = c(model_2a$results[90,2],
                         model_2b$results[86,2],
                         model_2c$results[49,2],
                         model_2d$results[79,2],
                         model_2e$results[74,2]),
             k = c(90, 86, 49, 79, 74))
models_1_2
```
KNN performed better than logistic regression in all 5 factor combinations, but not by much. In both models, Perimeter and Shape Factor 2 produced the best macro F1 scores at `models_1_2[3,3]`. For the next models, more variables will be included on top of those two to see if the macro F1 score can be improved further...

## Model 3: Decision Trees

-Start with Peri, SF2 (from models 1, 2)

```{r model3_Perimeter_ShapeFactor2}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3a$results
```

-Then add more variables (on paper, make a compatibility matrix of all non-group-0 variables based on formulae) -> choose good ones that are also compatible
```{r model3_Perimeter_ShapeFactor2_MajorAxisLength}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,3)], # col 2 for perimeter, 12 for SF2, 3 for maj ax
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3b$results
```
```{r model3_Perimeter_ShapeFactor2_Eccentricity}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,6)], # col 2 for perimeter, 12 for SF2, 3 for Eccentricity
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3c$results
```
```{r model3_Perimeter_ShapeFactor2_MajorAxisLEngth_Eccentricity}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3d <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,3,6)],
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3d$results
```
```{r four_factor_tree}
rpart.plot(model_3d$finalModel)
```


-multiple trees with 3-4 variables (if you can even find that many good ones)

-See which tree does best, then select that for RF

-Using all 4 appeared to be the best? Ask about plotting (plot too cluttered to be readable) while you do RF tomorrow

## Model 4: Random Forests

-Take the variables from the best DT, and run one (1) RF using ranger

## Validation Run

-pick ultimate model, use it to predict validation set

-report: mF1, confusion matrix, perhaps accuracy too

# Discussion

-TBD

# Conslusion

-TBD

# Citation

-TBD
