---
title: "R Data Science Course 9: Choose Your Own Project"
author: "James Lo"
date: '2022-08-30'
output: 
  pdf_document:
    latex_engine: xelatex
    df_print: kable
    toc : TRUE
    toc_depth: 3
urlcolor: blue
mainfont: Calibri Light
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

## Executive Summary

Machine learning has become increasingly [relevant in agriculture today](https://www.bayer.com/en/agriculture/article/machine-learning-uses-agriculture), as human population grows and researchers seek more efficient methods to meet demands for food on limited arable land. In particular, classifying large volumes of crops is important in modern agriculture in several contexts, including quality assessment after harvest and modeling the success of different crop varieties in various growing conditions. The use of machine learning algorithms for crop classification provides several advantages over manual approaches including higher throughput, superior accuracy and precision, and the development of more complex models capable of recognizing data trends that a human observer may have difficulty noticing. Focusing on the classification of different crop varieties, this project seeks to analyze the potential predictors in a previously generated dataset, before choosing a small collection of appropriate algorithms and evaluating the performances of the models generated.

This project is part of the [EdX Harvard R Data Science program](https://www.edx.org/professional-certificate/harvardx-data-science).

## Data Loading
```{r packages, warning = F, include = F}
if(!require(tidyverse)) install.packages("tidyverse",
                                         repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret",
                                     repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", 
                                          repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", 
                                     repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", 
                                      repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", 
                                         repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", 
                                       repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", 
                                       repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", 
                                       repos = "http://cran.us.r-project.org")
if(!require(viridis)) install.packages("viridis", 
                                       repos = "http://cran.us.r-project.org")
if(!require(MLmetrics)) install.packages("MLmetrics", 
                                       repos = "http://cran.us.r-project.org")
if(!require(class)) install.packages("class", 
                                       repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", 
                                       repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", 
                                       repos = "http://cran.us.r-project.org")
if(!require(ranger)) install.packages("ranger", 
                                       repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
library(dslabs)
library(dplyr)
library(lubridate)
library(ggplot2)
library(readr)
library(readxl)
library(viridis)
library(MLmetrics)
library(class)
library(rpart)
library(rpart.plot)
library(ranger)
```

The dataset selected was sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), under the name "[Dry Bean Dataset](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset)". An inspection of the webpage description revealed that the data was originally generated by [Koklu and Ozkan (2020)](https://www.sciencedirect.com/science/article/pii/S0168169919311573?via%3Dihub), for developing models to distinguish between seven dried bean varieties commonly produced in Turkey.

First, the dataset was downloaded from the URL (part of the repository website) to the working directory as a .zip file and subsequently unzipped. The unzipped folder was named `DryBeanDataset`.

```{r data_download, warning = FALSE}
uci_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip'

download.file(uci_url, 'DryBeanDataset.zip')
```
```{r data_unzip, warning = FALSE}
unzip('DryBeanDataset.zip')
```

Then, the first sheet of the Excel file (`Dry_Bean_Dataset.xlsx`) within the folder was extracted from its location within the working directory. The other sheet in the file contains a citation for the paper by Koklu and Ozkan (2020) which, while useful for a bibliography, is irrelevant for data analysis.

```{r data_read, warning = FALSE}
# specify path
path <- paste0(getwd(), '/DryBeanDataset/Dry_Bean_Dataset.xlsx')
# read the first sheet (where the data is)
beans <- read_xlsx(path, sheet = 1)
```

## Exploratory Analysis

Before models could be trained and evaluated, the dataset itself was examined to reveal its format, number of potential predictors, and other information required for efficient data manipulation. Both the UCI Machine Learning Repository page and the Koklu and Ozkan (2020) paper were used as references for information not immediately obvious from the dataset alone.

### Overview

```{r data_overview}
# overview of the data's size and format
str(beans)
```
The dataset contains 13,611 entries, each representing a scanned bean. There are 12 metrics describing the geometric features of each bean, plus four 'shape factors' which are derived from an [earlier machine learning article on rice grains](https://www.scinapse.io/papers/345887372) (Pazoki et al., 2014). All metrics except for `Class` (the outcome variable for this classification problem) are numeric, with `Class` being of class character.

A quick glance at the magnitudes of each numeric metric revealed large differences in scale; normalization will be needed for plotting and model fitting.

### Checking for NA Values

```{r na_check}
which(is.na(beans))
```
As described on the UCI repository webpage, the dataset indeed contains no NA entries.

### Distribution of Outcome Classes

```{r num_classes}
# checking the distribution of classes (bean types) in the dataset
beans %>% group_by(Class) %>%
  summarize(count = n())
```
**Table 1: Number of instances among the seven bean types (classes).**

The dataset has a rather uneven distribution of the 7 bean varieties. The most frequently found type is Dermason at 3546 entries, while the least common is Bombay at 522 entries. This uneven distribution must be accounted for during model fitting and accuracy assessments.

### Distribution of Potential Predictor Variables Among Classes

Before building models, the distribution of values among different bean varieties were visualized with boxplots for all 16 possible predictor variables. This was done to understand the levels of variability between bean varieties that were present in each potential predictor, and select the most informative predictors for models that perform best with lower-dimensional data.

As mentioned earlier, normalization was performed to produce all the plots featured within this report. While multiple formulae for normalization exist, the one used in this report is as follows:

$$X_{normalized} = \frac{(X - min)}{(max - min)}$$
Where $X$ is the value of a single instance within a metric, $max$ is the maximum value of the metric, and $min$ is the minimum value of the metric. This formula normalizes the values of each metric from 0 (minimum) to 1 (maximum).

*To conserve space, only the first instance in a series of very similar code chunks are displayed in the PDF file. A complete collection of code can be seen in the R Markdown and R files.*

#### Area

Koklu and Ozkan (2020) used a computer vision system (camera connected to a computer loaded with image-processing software) to scan each bean within the dataset. In their paper, the metric of Area was defined as the number of pixels within each scanned bean image, and denoted by $A$.

```{r Area_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Area > (quantile(Area, 0.75)) + 1.5 * IQR(Area) | 
           Area < (quantile(Area, 0.25)) - 1.5 * IQR(Area))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Area), y = Area)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Areas')
```
**Figure 1: Normalized distribution of Areas among the seven bean varieties.**

The area IQR for Bombay is distinctively higher than those of all other varieties. Heavily overlapping IQRs exist in Cali and Barbunya, as well as Sira and Seker. Dermason, Seker, and Sira all have much narrower IQRs compared to the other varieties, despite having very close distribution ranges. Most outliers are on the upper ends of their respective distributions.

#### Perimeter

The metric of Perimeter (denoted by $P$) was defined as the length of each bean image's border (Koklu and Ozkan, 2020).

```{r Perimeter_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Perimeter > (quantile(Perimeter, 0.75)) + 1.5 * IQR(Perimeter) | 
           Perimeter < (quantile(Perimeter, 0.25)) - 1.5 * IQR(Perimeter))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Perimeter), y = Perimeter)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Perimeters')
```
**Figure 2: Normalized distribution of Perimeters among the seven bean varieties.**

Similarly to the Area metric, Bombay has a distinctively higher range of Perimeter lengths than the other varieties. The IQR for Cali exists entirely within that of Barbunya, while the IQRs for Dermason and Seker overlap. The IQRs of perimeter lengths for smaller bean varieties are less tight than seen in the Area graph, but also somewhat more distinct relative to the scale of the y-axis.

#### Major Axis Length

The Major Axis Length ($L$) was defined as the "distance between the ends of the longest
line that can be drawn from a bean" (Koklu and Ozkan, 2020); in other words, the length of each bean.

```{r MajorAxisLength_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(MajorAxisLength > (quantile(MajorAxisLength, 0.75)) + 1.5 * IQR(MajorAxisLength) | 
           MajorAxisLength < (quantile(MajorAxisLength, 0.25)) - 1.5 * IQR(MajorAxisLength))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, MajorAxisLength), y = MajorAxisLength)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Major Axis Lengths')
```
**Figure 3: Normalized distribution of Major Axis Lengths among the seven bean varieties.**

For Major Axis Length, Bombay once again has an IQR that was distinctively larger than all others. Barbunya/Horoz and Dermason/Seker have heavily overlapping IQRs, while Cali has a smaller IQR overlap with Horoz/Barbunya. Seker and Sira hav more outliers on the high end of their ranges, while Horoz has more on the low end.

#### Minor Axis Length

The Minor Axis Length ($l$) was described as the "longest line that can be drawn from the
bean while standing perpendicular to the main axis" (Koklu and Ozkan, 2020); it can be thought of as the width of each bean.

```{r MinorAxisLength_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(MinorAxisLength > (quantile(MinorAxisLength, 0.75)) + 1.5 * IQR(MinorAxisLength) | 
           MinorAxisLength < (quantile(MinorAxisLength, 0.25)) - 1.5 * IQR(MinorAxisLength))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, MinorAxisLength), y = MinorAxisLength)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Minor Axis Lengths')
```
**Figure 4: Normalized distribution of Minor Axis Lengths among the seven bean varieties.**

The distribution of Minor Axis Lengths are similar to that of Major Axis lengths, but with a few differences: here the heaviest overlap is between Cali and Barbunya, followed by Horoz/Sira and Sira/Seker. Also, Dermason has a partial IQR overlap with Horoz rather than a near-complete one with Seker. Most outliers are on the high ends of their respective groups. Cali, Seker, and Horoz have more outliers on th high ends of their respective groups, while Sira has more on the low end.

#### Aspect Ratio

The Aspect Ratio ($K$) was given in the original paper as the ratio between the Major and Minor Axis lengths (Koklu and Ozkan, 2020); in other words:

$$K = \frac{L}{l}$$

```{r AspectRation_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(AspectRation > (quantile(AspectRation, 0.75)) + 1.5 * IQR(AspectRation) | 
           AspectRation < (quantile(AspectRation, 0.25)) - 1.5 * IQR(AspectRation))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, AspectRation), y =AspectRation)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Aspect Ratios')
```
**Figure 4: Normalized distribution of Aspect Ratios among the seven bean varieties.**

The Aspect Ratio distributions differ from the previous potential predictors in that Bombay no longer has the most distinct distribution of values. Instead, Horoz and Cali have IQRs distinctively higher than others, while Seker has a lower IQR. The remaining varieties (Dermason, Barbunya, Sira, Bombay) all have heavily overlapping IQRs. Horoz has more outliers on the bottom end of its distribution, while Sira, Dermason, and Seker have more on their high ends.

#### Eccentricity

[Eccentricity](https://www.cuemath.com/geometry/eccentricity/) is a measure describing the shape of a[conic section](https://www.cuemath.com/geometry/conic-sections/). The eccentricity of each bean ($Ec$) was defined by Koklu and Ozkan (2020) as being the "eccentricity of an ellipse having the same [moments](https://byjus.com/jee/moment-of-inertia-of-ellipse/) as the [bean image] region".

The [eccentricity of an ellipse](https://www.cuemath.com/geometry/eccentricity-of-ellipse/) is in turn a ratio of two values: **the distance to any point on the ellipse from the** [focus](https://www.cuemath.com/geometry/foci-of-ellipse/), and **the distance to that same point on the ellipse from the** [directrix](https://www.cuemath.com/geometry/directrix-of-ellipse/).

```{r Eccentricity_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Eccentricity > (quantile(Eccentricity, 0.75)) + 1.5 * IQR(Eccentricity) | 
           Eccentricity < (quantile(Eccentricity, 0.25)) - 1.5 * IQR(Eccentricity))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Eccentricity), y = Eccentricity)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Eccentricities')
```
**Figure 5: Normalized distribution of Eccentricities among the seven bean varieties.**

The Eccentricity distributions are similar to that of Aspect Ratios, but with the two varieties having higher IQRs (Horoz, Cali) deviating from the others less while the one with lower IQR (Seker) deviating more. Most outliers are on the low ends of their respective ranges.

#### Convex Area

Koklu and Ozkan (2020) defined the Convex Area ($C$) of a bean as the "number of pixels in the smallest convex polygon
that can contain the area of a bean seed".

```{r ConvexArea_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ConvexArea > (quantile(ConvexArea, 0.75)) + 1.5 * IQR(ConvexArea) | 
           ConvexArea < (quantile(ConvexArea, 0.25)) - 1.5 * IQR(ConvexArea))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ConvexArea), y = ConvexArea)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Convex Areas')

```
**Figure 6: Normalized distribution of Convex Areas among the seven bean varieties.**

With the Covex Area Sizes, a distribution somewhat similar to the Area, Perimeter, and Axis Length distributions could be seen again. Bombay has a distinctively high IQR of Convex Areas, while Dermason has the lowest range. Cali and Barbunya have heavily overlapping IQRs, with less overlap existing among the other varieties. The narrowest IQRs belong to Seker and Sira. Outliers are biased towards the top end for Bombay, Cali, Barbunya, Sira, and Seker.

#### Equivalent Diameter

Koklu and Ozkan (2020) defined the Equivalent Diameter ($Ed$) as "the diameter of a circle having the same
area as a bean seed area".

```{r EquivDiameter_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(EquivDiameter > (quantile(EquivDiameter, 0.75)) + 1.5 * IQR(EquivDiameter) | 
           EquivDiameter < (quantile(EquivDiameter, 0.25)) - 1.5 * IQR(EquivDiameter))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, EquivDiameter), y = EquivDiameter)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Equivalent Diameters')
```
**Figure 7: Normalized distribution of Equivalent Diameters among the seven bean varieties.**

The distributions of Equivalent Diameters are fairly similar to that of Convex Areas. Once again, Cali and Barbunya have heavy overlaps, with Bombay having a distinctly higher IQR. Cali and Seker have more outliers on the top ends of their distributions, while Horoz has more on the bottom end.

#### Extent

The Extent ($Ex$) of a bean is a metric for size, being defined as "ratio of the pixels in the bounding box to the bean
area" (Koklu and Ozkan, 2020). What constituted a "bounding box" was not specified in the paper, but given its [mathematical definition](https://www.mathopenref.com/coordbounds.html) it was presumably a rectangular background containing all points within the bean image.

```{r Extent_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Extent > (quantile(Extent, 0.75)) + 1.5 * IQR(Extent) | 
           Extent < (quantile(Extent, 0.25)) - 1.5 * IQR(Extent))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Extent), y = Extent)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Extents')
```
**Figure 8: Normalized distribution of Extents among the seven bean varieties.**

The distributions of Extent Ratios exhibit heavy IQR overlaps between all bean varieties. Among the three groups with outliers (Cali, Seker, Bombay), all have more on the bottom ends of their respective distributions.

#### Solidity

Solidity ($S$) is another area-related metric, being the "ratio of the pixels in the
convex shell to those found in beans" (Koklu and Ozkan, 2020). Note that "convex shell" does not appear to be a commonly used mathematical term; however, a similar term named "[convex hull](https://mathworld.wolfram.com/ConvexHull.html)" exists and the authors presumably meant the latter.

```{r Solidity_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Solidity > (quantile(Solidity, 0.75)) + 1.5 * IQR(Solidity) | 
           Solidity < (quantile(Solidity, 0.25)) - 1.5 * IQR(Solidity))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Solidity), y = Solidity)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Solidities')
```
**Figure 9: Normalized distribution of Solidities among the seven bean varieties.**

The distributions of Solidity values exhibit heavy IQR overlaps between nearly all bean varieties. Also, every bean variety has many outliers (values outside the boxplot whiskers) on the lower end of their respective distributions.

#### Roundness

Koklu and Ozkan (2020) gave the following formula for Roundness ($R$):

$$R = \frac{4\pi A}{P^2}$$
```{r roundness_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(roundness > (quantile(roundness, 0.75)) + 1.5 * IQR(roundness) | 
           roundness < (quantile(roundness, 0.25)) - 1.5 * IQR(roundness))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, roundness), y = roundness)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Roundnesses')
```
**Figure 10: Normalized distribution of Roundness values among the seven bean varieties.**

The ranking of median Roundness values are similar to that of that of Solidity, but with the lowest three (Cali, Barnunya, Horoz) switched. Less overlaps exist than in Solidity (except for heavy overlap Horoz and Barbunya), but many outliers outside the whiskers exist for all varieties, and are biased towards the low end of the range for all groups except Bombay.

#### Compactness

Compactness ($CO$) is another metric for bean roundness, and was given by Koklu and Ozkan (2020) as:

$$CO = \frac{Ed}{L}$$

```{r Compactness_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Compactness > (quantile(Compactness, 0.75)) + 1.5 * IQR(Compactness) | 
           Compactness < (quantile(Compactness, 0.25)) - 1.5 * IQR(Compactness))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Compactness), y = Compactness)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Compactness Values')
```
**Figure 11: Normalized distribution of Compactness values among the seven bean varieties.**

The distributions of Compactness values have heavy overlaps between Bombay, Sira, Barbunya, and Dermason. Seker has the highest range of Compactness, while Horoz has the lowest. All varieties have many outliers, mostly on the high ends of the distributions. Seker has most of its outliers on the low end of its range; the opposite was true for Bombay, Cali, and Horoz.

#### Shape Factor 1

In addition to the metrics described above, Koklu and Ozkan (2020) also made use of four "Shape Factors", from another machine learning article by Pazoki et al. (2014).

Shape Factor 1 ($SF1$) was defined as follows:

$$SF1 = \frac{L}{A}$$

```{r ShapeFactor1_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor1 > (quantile(ShapeFactor1, 0.75)) + 1.5 * IQR(ShapeFactor1) | 
           ShapeFactor1 < (quantile(ShapeFactor1, 0.25)) - 1.5 * IQR(ShapeFactor1))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor1), y = ShapeFactor1)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 1')
```
**Figure 12: Normalized distribution of SF1 values among the seven bean varieties.**

The distributions of Shape Factor 1 values feature heavy overlap between Barbunya/Cali, and relatively less overlap between Seker/Sira/Horoz. Dermason has the highest range of Shape Factor 1, and Bombay the lowest. All groups have more outliers on the top ends of their ranges except for Seker (more on the low end) and Bombay (two outliers on the low end only).

#### Shape Factor 2

Shape Factor 2 ($SF2$) was derived with the formula:

$$SF2 = \frac{l}{A}$$

```{r ShapeFactor2_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor2 > (quantile(ShapeFactor2, 0.75)) + 1.5 * IQR(ShapeFactor2) | 
           ShapeFactor2 < (quantile(ShapeFactor2, 0.25)) - 1.5 * IQR(ShapeFactor2))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor2), y = ShapeFactor2)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 2')
```
**Figure 13: Normalized distribution of SF2 values among the seven bean varieties.**

The distributions of Shape Factor 2 values have relatively less IQR overlap compared to most other variables examined, with the 4 highest Shape Factor 2 ranges (Seker, Dermason, Sira, Barbunya) all having non-overlapping IRRs. Horoz and Cali have a large IQR overlap, but neither has overlapping IQRs with Bombay. However, outliers on the high ends of the ranges are common. With the exception of Seker, all groups have more outliers on the top ends of their ranges.

#### Shape Factor 3

Shape Factor 3 ($SF3$) was derived from the formula:

$$SF3 = \frac{A}{({\frac{L}{2}})^2\pi}$$

```{r ShapeFactor3_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor3 > (quantile(ShapeFactor3, 0.75)) + 1.5 * IQR(ShapeFactor3) | 
           ShapeFactor3 < (quantile(ShapeFactor3, 0.25)) - 1.5 * IQR(ShapeFactor3))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor3), y = ShapeFactor3)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 3')
```
**Figure 14: Normalized distribution of SF3 values among the seven bean varieties.**

The distributions of Shape Factor 3 values have heavy overlaps in Bombay, Sira, Barbunya, and Dermason. Seker had a distinctively higher IQR than the other varieties but also many low outliers; the opposite was true for Horoz. Except for Seker, all groups have more outliers on the low ends of their ranges.

#### Shape Factor 4

Shape Factor 4 ($SF4$) was derived from the formula:

$$SF3 = \frac{A}{({\frac{L}{2}})(\frac{l}{2})\pi}$$

```{r ShapeFactor4_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor4 > (quantile(ShapeFactor4, 0.75)) + 1.5 * IQR(ShapeFactor4) | 
           ShapeFactor4 < (quantile(ShapeFactor4, 0.25)) - 1.5 * IQR(ShapeFactor4))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor4), y = ShapeFactor4)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 4')
```
**Figure 15: Normalized distribution of SF4 values among the seven bean varieties.**

The distributions for Shape Factor 4 exhibit heavy overlaps between nearly all varieties, with the possible exception of Seker. In addition, every variety has many outliers on the low ends of their respective distributions. All groups have outliers on the low ends of their distributions; Horoz has an exceptionally long range of outlier values.

### Combining Predictors for Scatterplots

Some common classification models, such as KNN, do not perform efficiently with high-dimensional data due to a phenomenon known as the [Curse of Dimensionality](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/), where an increase in the amount of predictors (dimensions) raises the possibility of overfitting the data in addition to making the data points sparser in space, requiring [less flexible models](https://rafalab.github.io/dsbook/examples-of-algorithms.html#the-curse-of-dimensionality) to maintain the same data coverage. As such, it would be constructive to select a pair of "compatible" predictor variables for those models. The goal is to have each variable contain inter-group (between bean varieties) variances that the other does not have sufficient amounts of. Also, the variables should be independent, lacking in [multicollinearity](https://www.theanalysisfactor.com/linear-regression-analysis-3-common-causes-of-multicollinearity-and-what-do-to-about-them/) of measurements. For instance, if one variable in the pair is already derived from bean lengths, the other variable should not also contain bean lengths in its formula. In addition, each of the two variables should have as much distance (or failing that, the least amount of overlap) between its group IQRs as possible, with the least amount of outliers and skew. Some variables (like Area and Convex Area) exist over a far larger numerical scale than others; this could be solved through normalizing all variables, making relative distances along the y-axis (as seen on the boxplots) more relevant than raw numerical differences.

To identify promising combinations, the variables were roughly grouped into six categories, reflecting broad features in their boxplots:

* Group 0 ("don't bother"): Variables whose boxplots display heavily overlapping IQRs and outliers that are visibly more numerous (compared to the other variables in this dataset) and/or heavily skewed in distributions. Extent, Solidity, and Shape Factor 4 were placed under this category.

* Group 1 ("Bombay high, Dermason low"): Variables where Bombay have the highest IQR (usually by a relatively large margin), and Dermason the lowest. Other bean varieties tend to have less separate or even overlapping IQRs. Area, Perimeter, Major Axis, Minor Axis, Convex Area, and Equivalent Diameter all fell under this category.

* Group 2 ("Horoz high, Seker low"): Variables where Horoz has the highest IQR, and Seker the lowest. Again, other groups tend to have less separate or overlapping IQRs. Aspect Ratio and Eccentricity fell under this cetegory.

* Group 3 ("Seker high, Horoz low"): The opposite of group 2.  Roundness and Compactness fell under this category. Note that as described in the original paper, Roundness and Compactness are both measures of how round a bean is, albeit calculated by different formulae. Both are also, by definition, the opposite metric of Aspect Ratio (which measures how elongated a bean is through the ratio between length/Major Axis Length and width/Minor Axis Width).

* Group 4 ("Dermason high, Bombay low"): The opposite of group 1, at least in terms of the defining feature (arbitrarily assigned for quick grouping). Shape Factor 1 was the sole variable assigned to this category.

* Group 5 ("Seker high, Bombay low"): Shape Factor 2 was the only variable assigned under this group.

To conserve memory, Group 0 variables were removed via the following code:

```{r trim_group0, echo = F}
# trim off group 0 ("don't bother") variables, which will not be considered further
beans <-
  subset(beans, select = -c(Extent, Solidity, ShapeFactor4))
# checking for right deletion
head(beans, 3)
```
Among group 1 variables, Perimeter and Equivalent Diameter appeared to be the most suitable choices, as they both have relatively less overlaps between the "middle groups" representing varieties other than Bombay and Dermason. Perimeter has large overlaps in Cali/Barbunya and Seker/Dermason, while Equivalent Diameter has large overlaps in Cali/Barbunya and Sira/Seker. Observing the boxplots, Aspect Ratio, Compactness, Shape Factor 2, and Shape Factor 3 were selected as the best complements due to having higher variances lacking in the group 1 choices, in addition to relatively less numerous and skewed outliers. Equivalent Diameter is incompatible with Compactness (Equivalent Diameter divided by Major Axis length), Shape Factor 2 (Minimum Axis length divided by Area), and Shape Factor 3 (which is derived from Area and Major Axis length). Meanwhile, Perimeter is compatible with Aspect Ratio, Compactness, Shape Factor 2, and Shape Factor 3. Included below are five scatterplots to further visualize the variables' ability to model bean varieties as distinct clusters of data points.

```{r Perim_AR}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         ar_tf = (AspectRation - min(AspectRation)) / (max(AspectRation) - min(AspectRation))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = ar_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Perimeter and Aspect Ratio Combinations')
```
**Figure 16: Scatterplot of Normalized Perimeter and Aspect Ratio values, color-coded by bean varieties.**

```{r Perim_Comp, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         comp_tf = (Compactness - min(Compactness)) / (max(Compactness) - min(Compactness))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = comp_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Compactness (Normalized)') +
  ggtitle('Normalized Perimeter and Compactness Combinations')
```
**Figure 17: Scatterplot of Normalized Perimeter and Compactness values, color-coded by bean varieties.**

```{r Perim_SF2, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         sf2_tf = (ShapeFactor2 - min(ShapeFactor2)) / (max(ShapeFactor2) - min(ShapeFactor2))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = sf2_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Shape Factor 2 (Normalized)') +
  ggtitle('Normalized Perimeter and Shape Factor 2 Combinations')
```
**Figure 18: Scatterplot of Normalized Perimeter and Shape Factor 2 values, color-coded by bean varieties.**

```{r Perim_SF3, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         sf3_tf = (ShapeFactor3 - min(ShapeFactor3)) / (max(ShapeFactor3) - min(ShapeFactor3))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = sf3_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Shape Factor 3 (Normalized)') +
  ggtitle('Normalized Perimeter and Shape Factor 3 Combinations')
```
**Figure 19: Scatterplot of Normalized Perimeter and Shape Factor 3 values, color-coded by bean varieties.**

```{r ED_AR, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(ed_tf = (EquivDiameter - min(EquivDiameter)) / (max(EquivDiameter) - min(EquivDiameter)),
         ar_tf = (AspectRation - min(AspectRation)) / (max(AspectRation) - min(AspectRation))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = ed_tf, y = ar_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Equivalent Diameter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Equivalent Diameter and Aspect Ratio Combinations')
```
**Figure 20: Scatterplot of Normalized Equivalent Diameter and Aspect Ratio values, color-coded by bean varieties.**

In all 5 scatterplots, adding another variable improved separation of the bean Class clusters that were poorly resolved in the group 1 variables (Barbunya/Cali, Sira/Seker, Seker/Dermason). However, large overlaps could still observed, especially in Dermason/Sira and Barbunya/Cali. Although the separation between Barbunya and Cali had improved from the near complete overlap in Perimeter and Equivalent Diameter boxplots, the scatterplots still revealed that a sizeable portion of the Barbunya cluster had been covered by the Cali cluster. Cluster visualization would be useful for further determination of which variable combination had the least amounts of overlap, but such a procedure would involve visualizing already tuned clustering algorithms and defeat the purpose of exploratory analysis. As a result, all five variable combinations were chosen for model building.

# Methods

With the insights gained from exploratory analyses and the original paper, the following steps were taken to prepare then analyze the data:

## Data Formatting

Before training any algorithms, the data was first split into two parts: a set containing all the potential predictors (the other 15 variables besides `Class`) named `beans_x`, and a set containing the bean varieties (`Class`) only named `beans_y`. This was done to comply with the argument format of `caret::train`, the function used to train all models in this project.

```{r x_y_split}
beans_x <- subset(beans, select = -c(Class))
beans_y <- subset(beans, select = c(Class))
```

While `caret::train` is capable of processing predictor data in the dataframe format using the [formula](https://www.rdocumentation.org/packages/caret/versions/4.47/topics/train) `y ~ x1 + x2 + ...`, attempts to train algorithms using this input method resulted in repeated errors. As such, an alternative input method was chosen with the predictor entered in matrix format; `beans_x` was converted into matrix form to enable this decision.

```{r x_as_matrix}
beans_x <- as.matrix(beans_x)
```

As previously done in the scatterplots (see Introduction section), the predictor variables must be normalized for algorithms such as KNN to work effectively. Since the variables vary greatly in scale, using them unaltered would lead to biased models that favor variables that exist over larger scales, [regardless of their actual importance](https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff) in relation to the outcome variable.

```{r x_normalization}
# for every column, apply the range formula onto each cell value:
# new value = (value - column min) / (column max - column min)
max_predictors <- apply(beans_x, 2, max) # column maxima
min_predictors <- apply(beans_x, 2, min) # column minima
colranges <- max_predictors - min_predictors # column ranges
beans_x <- sweep(beans_x, 2, STAT = min_predictors, FUN = '-') # subtract minima
beans_x <- sweep(beans_x, 2, STAT = colranges, FUN = '/') # divide by col ranges
```
```{r testing_matrix_norm_ED_AR, include = F}
# reconstitute the data table
as.data.frame(beans_x) %>%
  mutate(Class = beans_y$Class) %>%
 # plot with coloring by bean variety (Class)
  ggplot(aes(x = EquivDiameter, y = AspectRation, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Equivalent Diameter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Equivalent Diameter and Aspect Ratio Combinations')
```

## Training and Validation

Simply using the entire dataset to train models incurs the risk of [overtraining](https://blog.roboflow.com/train-test-split/), where the model is well-fitted to the particular dataset used for training but generalizes poorly to new data of the same type. As such, the Dry Beans Dataset (both the predictor and outcome subsets) was split into two portions: a training set to train the models on, and a validation set simulating new data to evaluate the "real" performance of each model after training is complete.

Many different training:validation ratios have been proposed by machine learning resources online, with 90:10, 80:20, and 70:30 being especially common. Note that a trade-off exists between the sizes of training and validation sets due to the original dataset's finite size. While having a large pool of data for training is certainly beneficial as it reduces variance in the predicted outcomes, the validation dataset cannot get so small as to cause imprecise performance evaluation due to high variance in the [performance statistics](https://www.v7labs.com/blog/train-validation-test-set). 

With 13611 observations/beans, the Dry Bean Dataset is fairly large and allows for a somewhat more generous allocation of data to the validation set; [Baeldung CS](https://www.baeldung.com/cs/train-test-datasets-ratio) suggests a 70:30 split for small datasets containing less than 10000 observations, with a 80:20 split being applicable for most cases. As such, a training:validation split of 80:20 was selected for this project.

Note that in most online sources outside EdX, the comparison dataset used to gauge final model performance is called the "training set", and the comparison dataset used for tuning is the "vaidation set"; in short, the two terms are switched between EdX and outside sources. For the sake of consistency, this project uses the EdX naming conventions.

```{r data_split}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# creating the validation indeces
val_index <-
  createDataPartition(y = beans$Class,
                      times = 1,
                      p = 0.2,
                      list = F)
# split the data
# as the x-y split had been made earlier, the train-val split must be made for both x and y
beans_x_train <- beans_x[-val_index,]
beans_y_train <- beans_y[-val_index,]
beans_x_val <- beans_x[val_index,]
beans_y_val <- beans_y[val_index,]
```
Unlike with the previous project (MovieLens Rating Prediction), the models used in this project should not rely on the same titles being present in the training and validation (or testing) sets. As such, the `semi_join` function will not be used to ensure that everything in the validation set being present in the training set. However, it must still be confirmed that both sets have all 7 bean classes.

```{r val_set_dim}
dim(beans_x_val)
```
```{r val_set_classes}
as.data.frame(beans_y_val) %>%
  group_by(Class) %>%
  summarize(counts = n())
```
**Table 2: Distribution of bean varieties within the validation subset, split from the original Dry Beans Dataset.**

```{r train_set_rows}
dim(beans_x_train)
```
```{r train_set_classes}
as.data.frame(beans_y_train) %>%
  group_by(Class) %>%
  summarize(counts = n())
```
**Table 3: Distribution of bean varieties within the training subset, split from the original Dry Beans Dataset.**

With 2726 observations in the validation set and 10885 in the training set, the two portions add up to 13611 and no observations were lost during the split. Also, both sets have all 7 bean classes.

## Accuracy Metric

For a classification model, some [commonly used performance metrics](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/) include accuracy, confusion matrix, and F1 score. Due to the class imbalance within the Dry Bean Dataset (see Introduction section), training models on accuracy could lead to misleading results where high accuracy scores are produced through predictions that favor more common classes within the dataset at the expense of rarer ones; such a situation is known as the accuracy paradox, and is caused by the model making predictions based on the imbalanced data distribution alone rather than any connections to the predictors. 

[Common ways to combat the accuracy paradox](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/) include taking larger samples, using a different performance metric, and resampling the dataset. For the choice of accuracy metric, the F1 score was chosen for this project for two reasons: it does not involve alterations of the data through resampling techniques, and it is robust against class imbalances.

The F1 score is the [harmonic mean](https://www.investopedia.com/terms/h/harmonicaverage.asp) of two metrics: precision (percentage of true positive predictions among all positive predictions) and recall (percentage of true positive predictions among actual positives).

The formula for precision is written as:

$$Precision = \frac{TP}{TP + FP}$$
Where $TP$ stands for True Positive predictions and $FP$ stands for False Positive predictions (actually negative, predicted as positive).

And the formula for recall is written as:

$$Recall = \frac{TP}{TP + FN}$$

Where $FN$ stands for False Negatives, data instances that are actually positive but incorrectly predicted as negative.

The F1 score formula then computes the harmonic mean as written below:

$$F1 = 2\frac{(Precision)(Recall)}{Precision + Recall}$$

For a binary classification problem, one outcome class could be arbitrarily defined as the positive and the other, the negative. However, since the Dry Beans Dataset contains more than two outcome classes, the simple positive/negative distinction must be expanded upon to derive [multiclass F1 scores](https://www.baeldung.com/cs/multi-class-f1-score). Multiclass F1 is computed by averaging the F1 score for each outcome category; when computing the F1 score of each class, the class in question is considered a "positive" and all other classes "negatives". Both macro F1 (unweighted average) and micro F1 (average F1 weighted by the size of each class) are used as accuracy metrics; as the goal of choosing F1 scores in this project is to avoid biases favoring larger classes, macro F1 was chosen for turning the models. For the Dry Beans Dataset, the macro F1 formula is thus:

$$mF1 = \frac{\Sigma_{class = 1}^7 F1_{class}}{7}$$
Where there are 7 classes, each denoted as $class$ in the formula.

Two functions, `macro_f1` and `f1`, were written; the former was designed to be a custom summary function under `caret::trainControl` and the later, a helper function for calculating individual F1 scores that together make up the macro F1.

```{r custom_multiclass_f1}
# this func should take same args as a function called defaultSummary()
# data = a dataframe with columns 'obs' and 'pred' (observed/actual and predicted)
macro_f1 <- function(data, lev = NULL, model = NULL) {
  mF1 <- mean(f1(data$pred, data$obs)) # arithmetic mean for macro F1
  names(mF1) <- 'mF1'# names the function output
  return(mF1)
}

f1 <- function(predicted, actual) {
  mat <- as.matrix(table(predicted, actual)) # should make a table where rows are pred, cols are actual/obs
  precision <- diag(mat) / rowSums(mat) # true pos / all predicted pos
  recall <- diag(mat) / colSums(mat) # true pos / all actual pos
  f1 <- ifelse(precision + recall == 0,
                      0,
                      2 * (precision * recall) / (precision + recall))
  return(f1)
}
```

As seen above, `macro_f1` (and all custom summary functions written for `caret::trainControl`) accepts a dataframe `data` with two columns: `obs` for observed outcomes, and `pred` for predicted outcomes. `f1` converts the dataframe into a [confusion matrix](https://machinelearningmastery.com/confusion-matrix-machine-learning/) tallying correct and incorrect predictions, where rows represent predicted outcomes and columns represent actual or observed outcomes. In such a matrix, a row sum is equal to the total predicted instances of a class, while a column sum represents the total actual instances of a class; the diagonal vector is then a series of correct predictions or True Positives.

```{r test_matrix, include = F}
test_table <- data.table(obs = c(1, 1, 2, 2, 2, 3, 4),
           pred = c(1, 1, 2, 2, 1, 3, 4))

test_table
```

```{r table_test, include = F}
as.matrix(table(test_table$pred, test_table$obs))
```
```{r f1_test, include = F}
f1(test_table$pred, test_table$obs) # okay so the f1 function returns something... and the right answers to boot!
```
```{r macro_f1_test, include = F}
macro_f1(test_table) # the macro_f1 func as a whole works
```

## Cross-Validation

Regardless of the algorithm used, [machine learning contains some randomness](https://machinelearningmastery.com/randomness-in-machine-learning/) on several levels. Sampling and splitting the dataset are random processes, as are the ordering of data instances fed into an algorithm and the iterative building of a predictive model using the algorithm. Setting random seeds before running each model imparts reproducibility, but the reality that each performance metric from a trained model is effectively a sampling statistic with variance attached remains.

One common way to reduce variance in machine learning is [k-folds cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/), a resampling technique that divides the training set into $k$ folds. For each fold, the remaining data (outside the fold) is used to build a model, with the model accuracy tested using the data within the fold. The performance metrics of all $k$ folds are then averaged to give a metric with lower variance; this is done for each hyperparameter (or combination of hyperparameters) tested during model tuning and training. [Common values](https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/) of $k$ include 3, 5, and 10; while higher values of $k$ are possible, 10 was chosen for this project as a compromise between reducing variance and avoiding overly long computational times. The parameters for a 10-fold cross validation, with each fold being 10 % of the training set, was specified using `caret::trainControl` as seen in the code chunk below.

Note that with 10-fold cross-validation, each fold would be 8 % (10 % of 80 %) of the full beans dataset.

```{r xval_method}
# 10-fold cross-validation
tenfold_xval <-
  trainControl(method = 'cv',
               number = 10,
               p = 0.9,
               summaryFunction = macro_f1) # custom macro f1 func in progress
```

## Model Choices

Many algorithms are available in the `caret` package for multiclass classification problems such as the one dealt with in this project. To demonstrate familiarity with the EdX program content, the following model types were chosen due to their detailed coverage in the machine learning course of the program:

### Logistic Regression

Logistic regression is a type of regression model used to find predictor-outcome correlations (and make predictions from them) in cases where the outcome/dependent variable is discrete rather than continuous. As its name suggests, logistic regression differs from linear regression in its [use of the logistic function](https://machinelearningmastery.com/logistic-regression-for-machine-learning/), the basic form of which is written below:

$$f(x) = \frac{L}{1 + e^{-k(x-x_0)}}$$
This basic logistic function creates a sigmoid curve whose $f(x)$ value goes from approaching 0 to approaching $L$, with an inflection point at $x = x_0$. When applied to regression, the formula becomes this:

$$y = Pr(y = + | X = x) = \frac{e^{b_0 + b_1x}}{1 + e^{b_0 + b_1x}}$$
With $b_0$ being the intercept and $b_1$ being the coefficient for the variable $x$, like in linear regression. Unlike in linear regression, $y$ in logistic regression is not the outcome variable itself but the *probability* of $y$ turning out to be a given class (arbitrarily denoted as $+$ in the formula above) given a certain value of predictor $X$ (denoted as lower case $x$).

Then using this function:

$$g(y) = ln\frac{y}{1 - y}$$
The logistic regression function is made linear again, as shown below:

$$g(y) = g(Pr(y = + | X = x)) = b_0 + b_1$$
Where $g(y)$ is the natural log of the [odds ratio](https://psychscenehub.com/psychpedia/odds-ratio-2/), for the probability of $y$ being class $+$ given $X = x$.

For this project, five pairs of predictors (the ones visualized on the scatterplots from figures 16-20, in the Introduction section) will be used build models using logistic regression to circumvent the curse of dimensionality, with the highest-performing one being the representative for the algorithm. As mentioned in the introduction section, the five pairs were chosen due to their ability to complement each others' lack of variance among certain classes, and lack of any redundancy in their derivation which would result in multicollinearity.

Since the `glm` method in `caret::train` (the usual method for training logistic regression models) is suited for binary classification only, the method `multinom` was used instead. `multinom` allows for the tuning of decay, [a regularization term](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/) that penalizes variables with less contributions to the model. As only pairs of variables were selected and fed into the logistic regression algorithm, decay was set to 0.

### K-Nearest Neighbors (KNN)

[The KNN algorithm](https://learn.g2.com/k-nearest-neighbor) is a classification technique based on processes that are intuitive to understand. For each unclassified data point, the algorithm examines a set amount (the namesake $k$; not to be confused with the one from k-folds cross-validation) of nearest classified data points, before classifying the data point into the most represented class among the $k$ nearest neighbors; for instance, if $k = 10$ and Seker makes up 7 of the 10 neighbors, the new data point would be classified as Seker. [Euclidean distance](https://www.cuemath.com/euclidean-distance-formula/) is the default metric for neighbor "nearness" (and the one used in this project), although other distance metrics exist.

For this project, the method `knn` under `caret::train` was used to train all KNN-based models. Like in Logistic regression, the five pairs of predictors from figures 16-20 were used to avoid the curse of dimensionality. $k$ itself was the only hyperparameter to be tuned, and an arbitrary range of 1 to 100 was used in an attempt to capture the optimal $k$ over a large span of possible values.

### Decision Tree

[The Decision Tree algorithm](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html) works by randomly selecting a series of thresholds (for instance, "is normalized perimeter greater than 0.5?") to partition data by, selecting the split (called a node) that leads to maximum homogeneity (uniformity of outcome class) within each branch, and recursively split the data with more sub-nodes and branches until no more sub-nodes could be made (either because some constraint is met or the sub-node has only one data point left). 

For this project, the method `rpart` under `caret::train` was used. [The only allowed tuning parameter](https://topepo.github.io/caret/available-models.html) under the `caret` version of `rpart` is [Complexity Parameter](https://www.learnbymarketing.com/tutorials/rpart-decision-trees-in-r/) (CP), a measure of how much improvement (in terms of reducing incorrect classifications) must be achieved for a node to be made. A CP range of 0.005 to 0.050 (in intervals of 0.001) was used to explore smaller CP values (the [default CP value](https://www.rdocumentation.org/packages/rpart/versions/4.1.16/topics/rpart.control) is 0.01) for the standalone `rpart` function) after initial test runs found that results were suboptimal at higher CP values; CP values below 0.005 were avoided to avoid overfitting (and overplotted trees too dense for R to visualize) caused by unrestrained node splitting. Unlike with logistic regression and KNN, additional compatible variables were added on top of the best-performing pair from the previous two algorithms to examine the performance of higher-dimensional models.

### Random Forest

Since decision trees often fit the training data rather closely, they are naturally prone to overfitting. [A random forest](https://www.ibm.com/cloud/learn/random-forest) mitigates this issue by building multiple independent decision trees, each using a randomly selected subset of the predictor features. The trees are then averaged to provide predictions.

For this project, [the method](https://cran.r-project.org/web/packages/ranger/ranger.pdf) `ranger` under `caret::train` was used for a faster creation of random forests. The tuning hyperparameters consist of `mtry` (how many features to consider at each split; this project tried a range from 1 to all the predictors available), `splitrule` (the homogeneity metric to evaluate goodness of split; [Gini impurity](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/) was chosen as the classification standard for `ranger`), and `min.node.size` (the minimal number of data points each node must have; an arbitrary range of 1-40 was chosen for coverage of the performance peak region after test runs).

Despite being designed to run random forests faster than most other algorithms, `ranger` still consumed large amounts of time due to the 10-fold cross-validation and large amount of trees making the computational volume extremely large for the laptop used in this project. Due to practical reasons, a threshold of 2 hours (considered the maximum acceptable wait time, given the situation of this project's author at the time of its creation) was set for the Random Forest. Only one combination (the best-performing one from the Decision Tree algorithm) of predictors were used, and the number of independent trees set to 250 (more trees were attempted, but resulted in run times exceeding 2 hours unless if the ranges of tuning hyperparameters were narrowed at the risk of possibly omitting optimal combinations).

# Results

## Model 1: Logistic Regression

```{r model1_Perimeter_AspectRation, results = 'hide'}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,5)], # col 2 for perimeter, 5 for aspect ratio
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric
```

```{r model_1a_results}
model_1a$results
```
**Table 4: Macro F1 score of Model 1a (Logistic Regression from Perimeter and Aspect Ratio).**

Logistic Regression using Perimeter and Aspect Ratio returned a macro F1 score of `0.8857739`, with a standard deviation of `0.01525563`.

```{r model1_Perimeter_Compactness, include = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_1b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,10)], # col 2 for perimeter, 10 for compactness
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric
```
```{r model_1b_results, echo = F}
model_1b$results
```
**Table 5: Macro F1 score of Model 1b (Logistic Regression from Perimeter and Compactness).**

Logistic Regression using Perimeter and Compactness returned a macro F1 score of `0.8903862	`, with a standard deviation of `0.01538134`.

```{r model1_Perimeter_ShapeFactor2, include = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_1c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric
```
```{r model_1c_results, echo = F}
model_1c$results
```
**Table 6: Macro F1 score of Model 1c (Logistic Regression from Perimeter and Shape Factor 2).**

Logistic Regression using Perimeter and Shape Factor 2 returned a macro F1 score of `0.8974943`, with a standard deviation of `0.009579192`.

```{r model1_Perimeter_ShapeFactor3, include = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_1d <-
  train(y = beans_y_train$Class,
                x =  beans_x_train[,c(2,13)], # col 2 for perimeter, 13 for SF3
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric
```
```{r model_1d_results, echo = F}
model_1d$results
```
**Table 7: Macro F1 score of Model 1d (Logistic Regression from Perimeter and Shape Factor 3).**

Logistic Regression using Perimeter and Shape Factor 3 returned a macro F1 score of `0.890064`, with a standard deviation of `0.01603693`.

```{r model1_EquivDiameter_AspectRation, include = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_1e <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(8,5)], # col 8 for equiv. diam., 5 for aspect ratio
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric
```
```{r model_1e_results, echo = F}
model_1e$results
```
**Table 8: Macro F1 score of Model 1e (Logistic Regression from Equivalent Diameter and Aspect Ratio).**

Logistic Regression using Equivalent Diameter and Aspect Ratio returned a macro F1 score of `0.8820995`, with a standard deviation of `0.01410419`.

## Model 2: K-Nearest Neighbors (KNN)

```{r model2_Perimeter_AspectRation, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_2a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,5)], # col 2 for perimeter, 5 for aspect ratio
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2a, highlight = T)
```
**Figure 21: Training macro F1 scores for Model 2a (Perimeter and Aspect Ratio) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 90` (within the diamond label) resulted in the highest macro F1 score, although all results from `k = 25` onwards are similarly high in macro F1.

```{r model_2a_besttune}
# model_2a$bestTune # k = 90 is best for this one
model_2a$results[90,]
```
**Table 9: Macro F1 score of Model 2a (KNN from Perimeter and Aspect Ratio).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.8820995`, with a standard deviation of `0.01410419`.

```{r model2_Perimeter_Compactness, echo = F, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_2b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,10)], # col 2 for perimeter, 10 for compactness
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2b, highlight = T)
```
**Figure 22: Training macro F1 scores for Model 2b (Perimeter and Compactness) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 86` (within the diamond label) resulted in the highest macro F1 score, although all results from `k = 25` onwards are similarly high in macro F1.

```{r model_2b_besttune, echo = F}
# model_2b$bestTune # k=86
model_2b$results[86,]
```
**Table 10: Macro F1 score of Model 2b (KNN from Perimeter and Compactness).**

KNN using Perimeter and Compactness returned a macro F1 score of `0.8908325`, with a standard deviation of `0.01170298`.

```{r model2_Perimeter_ShapeFactor2, echo = F, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_2c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2c, highlight = T)
```
**Figure 23: Training macro F1 scores for Model 2c (Perimeter and Shape Factor 2) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 49` (within the diamond label) resulted in the highest macro F1 score, although all results from `k = 25` onwards are similarly high in macro F1. Unlike with the previous two KNN models, mF1 scores exhibited a slight and gradula decrease after `k = 49`.

```{r model_2c_besttune, echo = F}
# model_2c$bestTune # k = 49
model_2c$results[49,]
```
**Table 11: Macro F1 score of Model 2c (KNN from Perimeter and Shape Factor 2).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.9000603`, with a standard deviation of `0.009347538`.

```{r model2_Perimeter_ShapeFactor3, echo = F, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_2d <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,13)], # col 2 for perimeter, 13 for SF3
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2d, highlight = T)
```
**Figure 24: Training macro F1 scores for Model 2d (Perimeter and Shape Factor 3) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 79` (within the diamond label) resulted in the highest macro F1 score. Unlike with the previous KNN models, the plateauing of mF1 values occurred slightly later at a `k` value of 30-40.

```{r model_2d_besttune, echo = F}
# model_2d$bestTune # k=79
model_2d$results[79,]
```
**Table 12: Macro F1 score of Model 2d (KNN from Perimeter and Shape Factor 3).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.8907138`, with a standard deviation of `0.01205434`.

```{r model2_EquivDiameter_AspectRation, echo = F, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

model_2e <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(8,5)], # col 8 for equiv. diam., 5 for aspect ratio
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2e, highlight = T)
```
**Figure 25: Training macro F1 scores for Model 2e (Equivalent Diameter and Aspect Ratio) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 74` (within the diamond label) resulted in the highest macro F1 score, although most mF1 values after `k = 25` are similarly high.

```{r model_2e_besttune, echo = F}
# model_2e$bestTune # k=74
model_2e$results[74,]
```
**Table 13: Macro F1 score of Model 2e (KNN from Equivalent Diameter and Aspect Ratio).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.884044`, with a standard deviation of `0.01043572`.

## Comparing Two-Variable Model Results

```{r models_1_2_table, echo = F}
models_1_2 <-
  data.table(variables = c('Perimeter, Aspect Ratio',
                           'Perimeter, Compactness',
                           'Perimeter, Shape Factor 2',
                           'Perimeter, Shape Factor 3',
                           'Equivalent Diameter, Aspect Ratio'),
             logreg_mF1_unregularized = c(model_1a$results[,2],
                            model_1b$results[,2],
                            model_1c$results[,2],
                            model_1d$results[,2],
                            model_1e$results[,2]),
             knn_mF1 = c(model_2a$results[90,2],
                         model_2b$results[86,2],
                         model_2c$results[49,2],
                         model_2d$results[79,2],
                         model_2e$results[74,2]),
             k = c(90, 86, 49, 79, 74))
models_1_2
```
**Table 14: A summary of macro F1 values from all five of the predictor pairs tested for both Logistic Regression and KNN, plus the k value used to obtain the KNN results.**

KNN performed better than logistic regression in all 5 factor combinations, but not by much. In both models, Perimeter and Shape Factor 2 produced the best macro F1 scores (`0.9000603` for KNN, `0.8974943` for Logistic Regression). For the Decision Tree models, more variables will be included on top of those two to see if the macro F1 score can be improved further.

## Model 3: Decision Trees

To start off the Decision Tree models, the predictor combination of Perimeter and Shape Factor 2 (which performed optimal results with Logistic Regression and KNN) was used to tune and build a Decision Tree model.

```{r model3_Perimeter_ShapeFactor2, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.005, 0.050, 0.001)

model_3a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

head(model_3a$results, 10)
```
**Table 15: First 10 tuning results for Model 3a (Decision Tree from Perimeter and Shape Factor 2), with Complexity Parameters, macro F1 values, and standard deviations.**

For Model 3a (Decision Tree using Perimeter and Shape Factor 2), a CP of `0.005` produced the highest macro F1 (`0.8559795`) with a standard deviation of `0.013672321	`.

The subsequent Decision Trees were run with additional predictors included on top of Perimeter and Shape Factor 2. As mentioned in the Introduction section, perdictors whose formulae involve $P$ (Perimeter), $l$ (Minor Axis Length), and $A$ (Area) will cause multicollinearity when used with Perimeter and Shape Factor 2 (SF2 being derived from dividing $l$ by $A$). Referring back to the original paper by Koklu and Ozkan (2020), only Major Axis Length ($L$) and Eccentricity were compatible as additional predictors. Compactness is derived from Equivalent Diameter which contains Area, while Convex Area is defined in relation to Area despite having no formula listed; the remaining perdictors all contained either one or multiple of $P$, $l$, and $A$.

Below are three more Decision trees; two of them have one additional predictor (Major Axis Length or Eccentricity), while the third one has four (both Major Axis Length and Eccentricity on top of Perimeter and SF2).

```{r model3_Perimeter_ShapeFactor2_MajorAxisLength, echo = F, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.005, 0.050, 0.001)

model_3b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,3)], # col 2 for perimeter, 12 for SF2, 3 for maj ax
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

head(model_3b$results, 10)
```
**Table 16: First 10 tuning results for Model 3b (Decision Tree from Perimeter, Shape Factor 2, and Major Axis Length), with Complexity Parameters, macro F1 values, and standard deviations.**

For Model 3b, a CP of `0.005` produced the highest macro F1 (`0.8658147`) with a standard deviation of `0.01590596`.

```{r model3_Perimeter_ShapeFactor2_Eccentricity, echo = F, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.005, 0.050, 0.001)

model_3c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,6)], # col 2 for perimeter, 12 for SF2, 6 for Eccentricity
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

head(model_3c$results, 10)
```
**Table 17: First 10 tuning results for Model 3c (Decision Tree from Perimeter, Shape Factor 2, and Eccentricity), with Complexity Parameters, macro F1 values, and standard deviations.**

For Model 3c, a CP of `0.005` produced the highest macro F1 (`0.8674779`) with a standard deviation of `0.008666564`. CPs from `0.006` to `0.041` all returned a mF1 of `0.8652256	`.

```{r model3_Perimeter_ShapeFactor2_MajorAxisLEngth_Eccentricity, echo = F, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.005, 0.050, 0.001)

model_3d <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,3,6)],
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

head(model_3d$results, 10)
```
**Table 18: First 10 turning results for Model 3d (Decision Tree from Perimeter, Shape Factor 2, Major Axis Length, and Eccentricity), with Complexity Parameters, macro F1 values, and standard deviations.**

For Model 3d, a CP of `0.005` produced the highest macro F1 (`0.8673091`) with a standard deviation of `0.009915875`. CPs from `0.008` to `0.050` all returned a mF1 of `0.8549304`.

The trained Decision Tree with the best performance was thus Model 3c with a CP of `0.005`. Below is a visualization of the tree:

```{r Perimeter_ShapeFactor2_Eccentricity}
rpart.plot(model_3c$finalModel)
```
**Figure 21: The trained Decision Tree (Model 3c) using variables Perimeter, Shape Factor 2, and Eccentricity with CP = 0.05.**

Using `rpart.plot` to generating a diagram of the Model 3c tree, the criteria for splitting (bold inequality statements under the non-terminal nodes), the percentage of total training data accounted for (the percentage at the bottom of each node), and the probabilities of different bean classes under each node (the series of decimals on the middle, between the bean class names and data percentage). The order of classes for the decimal probabilities are: Barbunya, Bombay, Cali, Dermason, Horoz, Seker, and Sira. The left branch to each non-terminal node represents data points that return `TRUE` for the splitting criteria, and the right branch represents data returning `FALSE` to the same statement. For example, the left-most terminal (Perimeter >= 0.66 AND Eccentricity < 0.9) predicted bean variety Bombay (which accounts for 4 % of the training data) with 100 % acuracy (probability of Bombay = 1.00). Bombay, Horoz, and Seker had the highest accuracies, while Cali had the lowest.

## Model 4: Random Forests

With the optimal combination of Decision Tree input predictors found, the following Random Forest was tuned:

```{r model4, message = F}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# RF tuning grid
forest_grid <-
  expand.grid(
    mtry = 1:3, # how many variables to use for splitting at each tree?
    splitrule = 'gini', # for classification. Not changing this
    min.node.size = 1:40 
  )

model_4 <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,6)],
        method = 'ranger',
        tuneGrid = forest_grid,
        num.trees = 250, # rough runtime of 2 hrs
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_4$results[85:95,]
```
**Table 19: Truncated Random Forest (Model 4) tuning results using Ranger, with predictors Perimeter, Shape Factor 2, and Eccentricity.**

The optimal Model 4 tuning hyperparameters were `mtry = 3` and `min.node.size = 10`. The resulting macro F1 was `0.9110494` with a standard deviation of `0.009791189`.

```{r model4_final_model, include = F}
model_4$bestTune
```

```{r model_4_plot}
plot(model_4) 
```
**Figure 22: mF1 values over different minimal node sizes for mtry = 1 (blue), mtry = 2 (magenta), and mtry = 3 (green).**

The trend lines for Random Forest mf1 values over minimal node sizes were rather jagged (indicating high variance); however, growing more trees would result in an unacceptably long run time.

## Validation Run

While model 4 (the sole Random Forests model) returned the highest training mF1 score, it would be worthwhile to perform validation runs on the representatives of all algorithm used not just to check for overfitting, but build confusion matrices to compare class-specific prediction weaknesses (if any are present) in each model. For Decision Trees, the basic 2-variable model was also validated alongside the optimized 3-variable model to compare their performances.

```{r validation_setup}
# convert beans_y_val$Class to class: Factor
beans_y_val$Class <- as.factor(beans_y_val$Class) # convert to factor class
levels(beans_y_val$Class) # checking that format is right
```

```{r validation_LR, include = F}
validation_preds_LR <- predict(model_1c$finalModel, beans_x_val)

confusionMatrix(validation_preds_LR, beans_y_val$Class)
```

```{r LR F1s, include = F}
# manual calculation of precision and recall values from confusion matrix
LR_precision_BAR <- 216 / (216 + 18 + 1 + 2)
LR_recall_BAR <- 216 / (216 + 41 + 3 + 5)
LR_precision_BOM <- 105 / 105
LR_recall_BOM <- 105 / 105
LR_precision_CAL <- 300 / (300 + 41 + 20 + 1)
LR_recall_CAL <- 300 / (300 + 18 + 5 + 3)
LR_precision_DER <- 648 / (648 + 7 + 12 + 43)
LR_recall_DER <- 648 / (648 + 1 + 12 + 49)
LR_precision_HOR <- 352 / (352 + 5 + 1 + 17)
LR_recall_HOR <- 352 / (352 + 1 + 20 + 7 + 6)
LR_precision_SEK <- 377 / (377 + 3 + 12 + 11)
LR_recall_SEK <- 377 / (377 + 12 + 17)
LR_precision_SIR <- 454 / (454 + 5 + 3 + 49 + 6 + 17)
LR_recall_SIR <- 454 / (454 + 2 + 1 + 43 + 17 + 11)

# F1 func from matrix
f1_from_matrix <- function(precision, recall) {
  score <- 2 * (precision * recall) / (precision + recall)
  return(score)
}

# F1 values
LR_F1_BAR <- f1_from_matrix(LR_precision_BAR, LR_recall_BAR)
LR_F1_BOM <- f1_from_matrix(LR_precision_BOM, LR_recall_BOM)
LR_F1_CAL <- f1_from_matrix(LR_precision_CAL, LR_recall_CAL)
LR_F1_DER <- f1_from_matrix(LR_precision_DER, LR_recall_DER)
LR_F1_HOR <- f1_from_matrix(LR_precision_HOR, LR_recall_HOR)
LR_F1_SEK <- f1_from_matrix(LR_precision_SEK, LR_recall_SEK)
LR_F1_SIR <- f1_from_matrix(LR_precision_SIR, LR_recall_SIR)
LR_F1_MACRO <- mean(c(LR_F1_BAR,
                      LR_F1_BOM,
                      LR_F1_CAL,
                      LR_F1_DER,
                      LR_F1_HOR,
                      LR_F1_SEK,
                      LR_F1_SIR))
```

```{r validation_KNN, include = F}
# KNN in R is not like the others, in that it fits and evaluates in one function
# Thus, a second training is done with the pre-tuned k value

# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')


validation_preds_KNN <- knn(train = beans_x_train[,c(2,12)],
                            cl = beans_y_train$Class,
                            test = beans_x_val[,c(2,12)],
                            k = 49) # KNN run on optimized k value tuned from caret::train

confusionMatrix(validation_preds_KNN, beans_y_val$Class)
```


```{r KNN F1s, include = F}
# manual calculation of precision and recall values from confusion matrix
KNN_precision_BAR <- 219 / (219 + 22 + 2)
KNN_recall_BAR <- 219 / (219 + 39 + 3 + 4)
KNN_precision_BOM <- 105 / 105
KNN_recall_BOM <- 105 / 105
KNN_precision_CAL <- 299 / (299 + 39 + 16 + 1)
KNN_recall_CAL <- 299 / (299 + 22 + 3 + 2)
KNN_precision_DER <- 648 / (648 + 7 + 7 + 15)
KNN_recall_DER <- 648 / (648 + 2 + 19 + 11)
KNN_precision_HOR <- 350 / (350 + 3 + 2 + 7)
KNN_recall_HOR <- 350 / (350 + 2 + 16 + 7 + 11)
KNN_precision_SEK <- 381 / (381 + 3 + 19 + 8)
KNN_recall_SEK <- 381 / (381 + 7 + 18)
KNN_precision_SIR <- 457 / (457 + 4 + 2 + 41 + 11 + 18)
KNN_recall_SIR <- 457 / (457 + 1 + 55 + 7 + 8)

# F1 values
KNN_F1_BAR <- f1_from_matrix(KNN_precision_BAR, KNN_recall_BAR)
KNN_F1_BOM <- f1_from_matrix(KNN_precision_BOM, KNN_recall_BOM)
KNN_F1_CAL <- f1_from_matrix(KNN_precision_CAL, KNN_recall_CAL)
KNN_F1_DER <- f1_from_matrix(KNN_precision_DER, KNN_recall_DER)
KNN_F1_HOR <- f1_from_matrix(KNN_precision_HOR, KNN_recall_HOR)
KNN_F1_SEK <- f1_from_matrix(KNN_precision_SEK, KNN_recall_SEK)
KNN_F1_SIR <- f1_from_matrix(KNN_precision_SIR, KNN_recall_SIR)
KNN_F1_MACRO <- mean(c(KNN_F1_BAR,
                      KNN_F1_BOM,
                      KNN_F1_CAL,
                      KNN_F1_DER,
                      KNN_F1_HOR,
                      KNN_F1_SEK,
                      KNN_F1_SIR))
```

```{r validation_2_factor_DT, include = F}
# DT preds take dataframes...
validation_preds_2DT <- predict(model_3a$finalModel, data.frame(beans_x_val), type = 'class')

confusionMatrix(validation_preds_2DT, beans_y_val$Class)
```

```{r DT2 F1s, include = F}
# manual calculation of precision and recall values from confusion matrix
DT2_precision_BAR <- 202 / (202 + 46 + 3 + 1 + 2)
DT2_recall_BAR <- 202 / (202 + 48 + 6 + 1 + 8)
DT2_precision_BOM <- 105 / 105
DT2_recall_BOM <- 105 / 105
DT2_precision_CAL <- 250 / (250 + 48 + 16)
DT2_recall_CAL <- 250 / (250 + 46 + 27 + 3)
DT2_precision_DER <- 661 / (661 + 7 + 23 + 58)
DT2_recall_DER <- 661 / (661 + 17 + 32)
DT2_precision_HOR <- 347 / (347 + 6 + 27 + 9)
DT2_recall_HOR <- 347 / (347 + 3 + 16 + 7 + 13)
DT2_precision_SEK <- 363 / (363 + 1 + 17 + 7)
DT2_recall_SEK <- 363 / (363 + 1 + 23 + 19)
DT2_precision_SIR <- 452 / (452 + 8 + 3 + 32 + 13 + 19)
DT2_recall_SIR <- 452 / (452 + 2 + 58 + 9 + 7)

# F1 values
DT2_F1_BAR <- f1_from_matrix(DT2_precision_BAR, DT2_recall_BAR)
DT2_F1_BOM <- f1_from_matrix(DT2_precision_BOM, DT2_recall_BOM)
DT2_F1_CAL <- f1_from_matrix(DT2_precision_CAL, DT2_recall_CAL)
DT2_F1_DER <- f1_from_matrix(DT2_precision_DER, DT2_recall_DER)
DT2_F1_HOR <- f1_from_matrix(DT2_precision_HOR, DT2_recall_HOR)
DT2_F1_SEK <- f1_from_matrix(DT2_precision_SEK, DT2_recall_SEK)
DT2_F1_SIR <- f1_from_matrix(DT2_precision_SIR, DT2_recall_SIR)
DT2_F1_MACRO <- mean(c(DT2_F1_BAR,
                      DT2_F1_BOM,
                      DT2_F1_CAL,
                      DT2_F1_DER,
                      DT2_F1_HOR,
                      DT2_F1_SEK,
                      DT2_F1_SIR))
```

```{r validation_3_factor_DT, include = F}
validation_preds_3DT <- predict(model_3c$finalModel, data.frame(beans_x_val), type = 'class')

confusionMatrix(validation_preds_3DT, beans_y_val$Class)
```

```{r DT3 F1s, include = F}
# manual calculation of precision and recall values from confusion matrix
DT3_precision_BAR <- 184 / (184 + 23 + 1 + 1)
DT3_recall_BAR <- 184 / (184 + 76 + 3 + 2)
DT3_precision_BOM <- 105 / 105
DT3_recall_BOM <- 105 / 105
DT3_precision_CAL <- 294 / (294 + 76 + 34 + 4)
DT3_recall_CAL <- 294 / (294 + 23 + 8 + 1)
DT3_precision_DER <- 645 / (645 + 7 + 19 + 61)
DT3_recall_DER <- 645 / (645 + 33 + 32)
DT3_precision_HOR <- 329 / (329 + 8 + 1)
DT3_recall_HOR <- 329 / (367 + 1 + 34 + 7 + 15)
DT3_precision_SEK <- 367 / (367 + 3 + 33 + 5)
DT3_recall_SEK <- 367 / (367 + 1 + 19 + 19)
DT3_precision_SIR <- 457 / (457 + 2 + 1 + 32 + 15 + 19)
DT3_recall_SIR <- 457 / (457 + 4 + 61 + 1 + 5)

# F1 values
DT3_F1_BAR <- f1_from_matrix(DT3_precision_BAR, DT3_recall_BAR)
DT3_F1_BOM <- f1_from_matrix(DT3_precision_BOM, DT3_recall_BOM)
DT3_F1_CAL <- f1_from_matrix(DT3_precision_CAL, DT3_recall_CAL)
DT3_F1_DER <- f1_from_matrix(DT3_precision_DER, DT3_recall_DER)
DT3_F1_HOR <- f1_from_matrix(DT3_precision_HOR, DT3_recall_HOR)
DT3_F1_SEK <- f1_from_matrix(DT3_precision_SEK, DT3_recall_SEK)
DT3_F1_SIR <- f1_from_matrix(DT3_precision_SIR, DT3_recall_SIR)
DT3_F1_MACRO <- mean(c(DT3_F1_BAR,
                      DT3_F1_BOM,
                      DT3_F1_CAL,
                      DT3_F1_DER,
                      DT3_F1_HOR,
                      DT3_F1_SEK,
                      DT3_F1_SIR))
```

```{r validation_RF, include = F}
validation_preds_RF <- predict(model_4$finalModel, beans_x_val)

confusionMatrix(validation_preds_RF$predictions, beans_y_val$Class)
```

```{r RF F1s, include = F}
# manual calculation of precision and recall values from confusion matrix
RF_precision_BAR <- 236 / (236 + 1 + 15 + 2 + 2)
RF_recall_BAR <- 236 / (236 + 23 + 3 + 3)
RF_precision_BOM <- 104 / 104
RF_recall_BOM <- 104 / 105
RF_precision_CAL <- 301 / (301 + 23 + 11)
RF_recall_CAL <- 301 / (301 + 15 + 7 + 3)
RF_precision_DER <- 656 / (656 + 6 + 9 + 55)
RF_recall_DER <- 656 / (656 + 3 + 16 + 35)
RF_precision_HOR <- 355 / (355 + 7 + 3 + 7)
RF_recall_HOR <- 355 / (355 + 2 + 11 + 6 + 12)
RF_precision_SEK <- 383 / (383 + 3 + 16 + 11)
RF_recall_SEK <- 383 / (383 + 9 + 14)
RF_precision_SIR <- 453 / (453 + 3 + 3 + 35 + 12 + 14)
RF_recall_SIR <- 453 / (453 + 2 + 55 + 7 + 11)

# F1 values
RF_F1_BAR <- f1_from_matrix(RF_precision_BAR, RF_recall_BAR)
RF_F1_BOM <- f1_from_matrix(RF_precision_BOM, RF_recall_BOM)
RF_F1_CAL <- f1_from_matrix(RF_precision_CAL, RF_recall_CAL)
RF_F1_DER <- f1_from_matrix(RF_precision_DER, RF_recall_DER)
RF_F1_HOR <- f1_from_matrix(RF_precision_HOR, RF_recall_HOR)
RF_F1_SEK <- f1_from_matrix(RF_precision_SEK, RF_recall_SEK)
RF_F1_SIR <- f1_from_matrix(RF_precision_SIR, RF_recall_SIR)
RF_F1_MACRO <- mean(c(RF_F1_BAR,
                      RF_F1_BOM,
                      RF_F1_CAL,
                      RF_F1_DER,
                      RF_F1_HOR,
                      RF_F1_SEK,
                      RF_F1_SIR))
```

## Final Tabulation of F1 Scores

```{r F1_tabulation, echo = F}
F1_table <-
  data.table(model = c('Logistic Regression',
                       'KNN',
                       'Decision Tree (2 Variables)',
                       'Decision Tree (3 variables)',
                       'Random Forest (2 variables)'),
             Barbunya = c(LR_F1_BAR,
                          KNN_F1_BAR,
                          DT2_F1_BAR,
                          DT3_F1_BAR,
                          RF_F1_BAR),
             Bombay = c(LR_F1_BOM,
                        KNN_F1_BOM,
                        DT2_F1_BOM,
                        DT3_F1_BOM,
                        RF_F1_BOM),
             Cali = c(LR_F1_CAL,
                      KNN_F1_CAL,
                      DT2_F1_CAL,
                      DT3_F1_CAL,
                      RF_F1_CAL),
             Dermason = c(LR_F1_DER,
                          KNN_F1_DER,
                          DT2_F1_DER,
                          DT3_F1_DER,
                          RF_F1_DER),
             Horoz = c(LR_F1_HOR,
                       KNN_F1_HOR,
                       DT2_F1_HOR,
                       DT3_F1_HOR,
                       RF_F1_HOR),
             Seker = c(LR_F1_SEK,
                       KNN_F1_SEK,
                       DT2_F1_SEK,
                       DT3_F1_SEK,
                       RF_F1_SEK),
             Sira = c(LR_F1_SIR,
                      KNN_F1_SIR,
                      DT2_F1_SIR,
                      DT3_F1_SIR,
                      RF_F1_SIR),
             Macro = c(LR_F1_MACRO,
                       KNN_F1_MACRO,
                       DT2_F1_MACRO,
                       DT3_F1_MACRO,
                       RF_F1_MACRO)
  )
F1_table
```
**Table 20: Tabulated validation F1 scores (by bean class and macro) of Logistic Regression, KNN, 2-variable Decision Tree (Perimeter and Shape Factor 2), 3-variable Decision Tree (Perimeter, SF2, Eccentricity), and Random Forest (ditto).**

Validating the highest-performing models (plus the two-variable Decision Tree for comparison), Random Forest with Perimeter, Shape Factor 2, and Eccentricity produced the best predictive performance in terms of macro F1 score. Interestingly, the 3-variable Random Forest was the only model that did not have perfect prediction for Bombay despite being the highest-performing model overall (in terms of macro F1) and in all bean varieties except Bombay and Sira. KNN provided the second best overall performance, followed by Logistic Regression, 2-variable Decision Tree, then 3-variable Decision Tree.

For Logistic Regression, KNN, and Random Forest, the weakest points were in Barbunya and Sira. Meanehile, the two weakest predictions in the two Decision Trees are in Barbunya and Cali. This is in line with discoveries from the exploratory analysis, where scatterplots generated from the starting variable pairs had large overlaps in Barbunya/Cali and Sira/Seker. As expected from the scatterplots (where Bombay consistently formed a cluster distant from all other classes), Bombay had the best predictions in all models tabulated.

*Individual confusion matrices were computed for each model, but are not included in the report PDF file to save space.*

# Discussion

## Summary of Results

The training and validation runs both revealed that Random Forest (with predictors Perimeter, Shape Factor 2, and Eccentricity) produced the highest F1 scores, followed by KNN, Logistic Regression, then Decision Tree. The addition of Eccentricity in the Decision Tree as a third predictor resulted in a higher training macro F1 score than just two predictors, adding Major Axis Length, or adding both Eccentricity and Major Axis Length; however, in validation the Decision Tree with two variables returned a higher macro F1 score than the 3-variable tree, implying that the 3-variable tree's superior performance during training might have been caused by overfitting.

The choice of the initial predictor pairs (a "group 1" variable paired with another variable outside the project-assigned group) during exploratory analysis influenced the performance of all machine learning models built in this project. Although the two "group 1" predictors chosen (Perimeter and Equivalent Diameter) had separate IQRs for most classes with less outliers than most other predictors (see boxplots from Introduction section), both of them had poor resolution between Barbunya/Cali (through the IQRs) and Sira/Seker (through the outliers). Combining those "group 1" predictors with other predictors that have better resolution between those classes (such as Shape Factor 2) mitigated the weakness somewhat, but still resulted in the class-specific lowered performances seen in Table 20.

## Limitations and Potential Future Solutions

### Perimeter Combinations

The choice of the initial predictor pair not only influenced model performance directly, but also determined what additional predictors could be added for higher-dimensional algorithms since many of the predictors in the dataset have shared terms in their derivation and therefore exhibit multicollinearity. With each predictor chosen, the list of allowed additional predictors for model-building narrowed; this was especially true for Shape Factor 2 (Minor Axis Length divided by Area), as many other dataset predictors were derived in relation to the area of each bean (see Introduction section and original peper by Koklu and Ozkan (2020) for formulae). If time permits, future attempts at similar problems could experiment with more variable combinations; however, depending on the dataset the amount of suitable predictor combinations (no multicollinearity and capable of resolving all outcome classes) may be similarly limited as the one used in this project.


### Statistical Significance of the Performance Evaluations

During algorithm training, it was found that many of the models (Random Forest/KNN, KNN/Logistic Regression, and the 2/3 variable Decision Trees) had standard deviations large enough for the training mF1s to overlap by adding and subtracting by their standard deviations (see Results section; not tabulated, but individual mF1s and SDs available). With such standard deviation values, it goes without calculating 95 % confidence intervals (which is +/- $1.96 / \sqrt{10}$ times the standard deviation, with $n = 10$ due to there being 10 folds and therefore 10 results to average) that statistically significant performance differences could not be determined from the training phase alone.

To increase model quality and decrease prediction variance during training, the most obvious solution would be a more generous training/validation dataset split in favor of the training set, using a ratio of 90:10 or perhaps even higher. However, as mentioned in the Methods section, overly high training/validation ratios would increase the variance of validation performance metrics instead, also hindering a proper assessment of model performances. To ensure a balance between training and validation precisions by examining them both, future machine learning projects could use `caret::train` or an equivalent function to perform another k-fold cross-validation for each validation, but with the tuning hyperparameters fixed at the combinations optimized from the training phase. Increasing the number of folds for cross-validation is another way to reduce variance while also improving model quality, although [some experimentation on the optimal fold number](https://cran.r-project.org/web/packages/cvms/vignettes/picking_the_number_of_folds_for_cross-validation.html) would have to be done as increasing fold numbers has diminishing returns. For the Random Forests algorithm in particular, more trees could be grown during training, but this is a computationally costly decision that could only be feasible if subsequent projects make use of computers with more memory and higher processing speeds.

Finally, the above options for decreasing variance and improving performance could be combined with other algorithms. The Support Vector Machine (SVM), used in the original paper by Koklu and Ozkan (2020), is a classification algorithm that [searches for optimal hyperplanes that can divide the data into groups, corresponding to different outcome classes](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html). Meanwhile, Singular Vector Decomposition (SVD) is a matrix factorization technique that uses linear algebra to [break down a data matrix into three simpler matrices](https://gregorygundersen.com/blog/2018/12/10/svd/) to [reveal latent features](https://pantelis.github.io/cs301/docs/common/lectures/recommenders/netflix/) that are then combined to make predictions. Principle Component Analysis (PCA), another matrix factorization technique, analyzes predictor covariances and [reduces high-dimensional data down to a smaller selection of principle components](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) that explain the maximum possible variance in the original data.

## Practical Implications of Results and Possible Future Directions

Refining means to classify large volumes of crops has many applications both on the marketing and agricultural side of the food economy. Koklu and Ozkan (2020) stated in their original paper that their models would be useful as a method of high-throughput quality assessment for planting and marketing alike. Beyond those applications, further improvements on projects such as this one could also be adapted with additional types of predictors and/or other crops for [identifying new traits or metrics that are more efficient for crop yield prediction](https://www.frontiersin.org/articles/10.3389/fpls.2020.624273/full), [more effective phenotyping of various crop cultivars for selective breeding purposes](https://www.frontiersin.org/articles/10.3389/fpls.2016.01864/full), and [envirotyping](https://link.springer.com/article/10.1007/s00122-016-2691-5) crops by modeling interactions between genotype, phenotype, and growing conditions both biotic and abiotic over time. As world population density grows and climate change continues to cause degradation of existing arable land, all of the above applications will prove important for farmers of the present and future.


# Citations

All referenced sources have been enclosed in links within the report document, and listed below:

[Machine Learning: How a Game of Checkers is Changing Agriculture](https://www.bayer.com/en/agriculture/article/machine-learning-uses-agriculture) from Bayer Global.

[The Professional Certificate in Data Science Program](https://www.edx.org/professional-certificate/harvardx-data-science) by Harvard on EdX, taught by Dr. Rafael Irizarry.

[The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) - Main Page.

[The UCI Repository Page for the Dry Beans Dataset](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset)

[Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques](https://www.sciencedirect.com/science/article/pii/S0168169919311573?via%3Dihub), scientific article by Murat Koklu and Ilker Ali Ozkan (2020).

[Classification of Rice Grain Varieties Using Two Artificial Neural Networks (MLP and neuro-fuzzy)](https://www.scinapse.io/papers/345887372), scientific article by Alireza Pazoki et al. (2014).

[Definition of Eccentricity](https://www.cuemath.com/geometry/eccentricity/) from Cuemath.

[Definition of Conic Sections](https://www.cuemath.com/geometry/conic-sections/) from Cuemath.

[Moment of Inertia of an Ellipse](https://byjus.com/jee/moment-of-inertia-of-ellipse/) from BYJU's.

[Eccentricity of Ellipse](https://www.cuemath.com/geometry/eccentricity-of-ellipse/) from Cuemath.

[Foci of Ellipse](https://www.cuemath.com/geometry/foci-of-ellipse/) from Cuemath.

[Directrix of Ellipse](https://www.cuemath.com/geometry/directrix-of-ellipse/) from Cuemath.

[Circumscribed Rectangle, or Bounding Box](https://www.mathopenref.com/coordbounds.html) from Math Open Reference.

[Convex Hull](https://mathworld.wolfram.com/ConvexHull.html) from Wolfram Mathworld

[The Curse of Dimensionality in Classification](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/) by Vincent Spruyt, for Computer Vision for Dummies.

[32.10.1 - The Curse of Dimensionality](https://rafalab.github.io/dsbook/examples-of-algorithms.html#the-curse-of-dimensionality) by Dr. Irizarry.

[Linear Regression Analysis  3 Common Causes of Multicollinearity and What Do to About Them](https://www.theanalysisfactor.com/linear-regression-analysis-3-common-causes-of-multicollinearity-and-what-do-to-about-them/) by Karen Grace-Martin from The Analysis Factor.

[The caret::train documentation](https://www.rdocumentation.org/packages/caret/versions/4.47/topics/train) from RDocumentation.org.

[How, When, and Why Should You Normalize / Standardize / Rescale Your Data?](https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff), blog post by the Editorial Team at Towards AI

[Train, Validation, Test Split for Machine Learning](https://blog.roboflow.com/train-test-split/) by Jacob Solawetz for Roboflow.

[Train Test Validation Split: How To & Best Practices 2022](https://www.v7labs.com/blog/train-validation-test-set) by Pragati Baheti for V7.

[Splitting a Dataset into Train and Test Sets](https://www.baeldung.com/cs/train-test-datasets-ratio) by A. Aylin Toku for Baeldung CS.

[Metrics to Evaluate Your Classification Model to Take the Right Decisions](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/) by Sumeet Kumar Argawal for Analytics Vidhya.

[8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/) by Jason Brownlee for Machine Learning Mastery.

[Harmonic Mean](https://www.investopedia.com/terms/h/harmonicaverage.asp) by Adam Hayes, reviewed by Khadija Khartit, and fact checked by Amanda Jackson for Investopedia.

[F-1 Score for Multiclass Classification](https://www.baeldung.com/cs/multi-class-f1-score) from Baeldung CS.

[What is a Confusion Matrix in Machine Learning](https://machinelearningmastery.com/confusion-matrix-machine-learning/) by Jason Brownlee for Machine Learning Mastery.

[Embrace Randomness in Machine Learning](https://machinelearningmastery.com/randomness-in-machine-learning/) by Jason Brownlee for Machine Learning Mastery.

[A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/) by Jason Brownlee for Machine Learning Mastery.

[How to Configure k-Fold Cross-Validation](https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/) by Jason Brownlee for Machine Learning Mastery.

[Logistic Regression for Machine Learning](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) by Jason Brownlee for Machine Learning Mastery.

[Odds Ratio](https://psychscenehub.com/psychpedia/odds-ratio-2/) on Psych Scene Hub.

[Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/) by kassambara on STHDA.

[What Is K-Nearest Neighbor? An ML Algorithm to Classify Data](https://learn.g2.com/k-nearest-neighbor) by Amal Joby for Learn Hub.

[Euclidean Distance Formula](https://www.cuemath.com/euclidean-distance-formula/) from Cuemath.

[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html) by Nagesh Singh Chauhan for KDnuggets.

[List of Available caret Models](https://topepo.github.io/caret/available-models.html) by Max Kuhn.

[Decision Tree Tutorial](https://www.learnbymarketing.com/tutorials/rpart-decision-trees-in-r/) from Learn by Marketing.

[Documentation for rpart.control](https://www.rdocumentation.org/packages/rpart/versions/4.1.16/topics/rpart.control) from RDocumentation.org.

[Random Forest](https://www.ibm.com/cloud/learn/random-forest) explanation from IBM Cloud Education.

[PDF of the ranger Documentation](https://cran.r-project.org/web/packages/ranger/ranger.pdf) by Marvin N. Wright, Stefan Wager, and Philipp Probst.

[Multiple-k: Picking the Number of Folds for Cross-Validation](https://cran.r-project.org/web/packages/cvms/vignettes/picking_the_number_of_folds_for_cross-validation.html) by Ludvig Renbo Olsen on cran.r-project.org.

[Support Vector Machines: A Simple Explanation](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html) by Noel Bambrick for KDnuggets.

[The Netflix Prize and Singular Value Decomposition](https://pantelis.github.io/cs301/docs/common/lectures/recommenders/netflix/), course material for the New Jersey Institute of Technology.

[A Step-by-Step Explanation of Principal Component Analysis (PCA)](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) by Zakaria Jaadi for Built In.

[Application of Machine Learning Algorithms in Plant Breeding: Predicting Yield From Hyperspectral Reflectance in Soybean](https://www.frontiersin.org/articles/10.3389/fpls.2020.624273/full), scientific article by Mohsen Yoosefzadeh-Najafabadi et al. (2021).

[Phenotyping: Using Machine Learning for Improved Pairwise Genotype Classification Based on Root Traits](https://www.frontiersin.org/articles/10.3389/fpls.2016.01864/full) by Jiangsan Zhao et al. (2016).

[Envirotyping for Deciphering Environmental Impacts on Crop Plants](https://link.springer.com/article/10.1007/s00122-016-2691-5), scientific article by Yunbi Xu (2016).