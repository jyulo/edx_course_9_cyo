---
title: "R Data Science Course 9: Choose Your Own Project"
author: "James Lo"
date: '2022-08-27'
output: 
  pdf_document:
    latex_engine: xelatex
    df_print: kable
    toc : TRUE
    toc_depth: 3
urlcolor: blue
mainfont: Calibri Light
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Introduction

## Executive Summary

Machine learning has become increasingly [relevant in agriculture today](https://www.bayer.com/en/agriculture/article/machine-learning-uses-agriculture), as human population grows and researchers seek more efficient methods to meet demands for food on limited arable land. In particular, classifying large volumes of crops is important in modern agriculture in several contexts, including quality assessment after harvest and modeling the success of different crop varieties in various growing conditions. The use of machine learning algorithms for crop classification provides several advantages over manual approaches including higher throughput, superior accuracy and precision, and the development of more complex models capable of recognizing data trends that a human observer may have difficulty noticing. Focusing on the classification of different crop varieties, this project seeks to analyze the potential predictors in a previously generated dataset, before choosing a small collection of appropriate algorithms and evaluating the performances of the models generated.

This project is part of the [EdX Harvard R Data Science program](https://www.edx.org/professional-certificate/harvardx-data-science).

## Data Loading
```{r packages, warning = F, include = F}
if(!require(tidyverse)) install.packages("tidyverse",
                                         repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret",
                                     repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", 
                                          repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", 
                                     repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", 
                                      repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", 
                                         repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", 
                                       repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", 
                                       repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", 
                                       repos = "http://cran.us.r-project.org")
if(!require(viridis)) install.packages("viridis", 
                                       repos = "http://cran.us.r-project.org")
if(!require(MLmetrics)) install.packages("MLmetrics", 
                                       repos = "http://cran.us.r-project.org")
if(!require(class)) install.packages("class", 
                                       repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", 
                                       repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", 
                                       repos = "http://cran.us.r-project.org")
if(!require(ranger)) install.packages("ranger", 
                                       repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
library(dslabs)
library(dplyr)
library(lubridate)
library(ggplot2)
library(readr)
library(readxl)
library(viridis)
library(MLmetrics)
library(class)
library(rpart)
library(rpart.plot)
library(ranger)
# will add more packages as needed

```

The dataset selected was sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), under the name "[Dry Bean Dataset](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset)". An inspection of the webpage description revealed that the data was originally generated by [Koklu and Ozkan (2020)](https://www.sciencedirect.com/science/article/pii/S0168169919311573?via%3Dihub), for developing models to distinguish between seven dried bean varieties commonly produced in Turkey.

First, the dataset was downloaded from the URL (part of the repository website) to the working directory as a .zip file and subsequently unzipped. The unzipped folder was named `DryBeanDataset`.

```{r data_download, warning = FALSE}
uci_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/00602/DryBeanDataset.zip'

download.file(uci_url, 'DryBeanDataset.zip')
```
```{r data_unzip, warning = FALSE}
unzip('DryBeanDataset.zip')
```

Then, the first sheet of the Excel file (`Dry_Bean_Dataset.xlsx`) within the folder was extracted from its location within the working directory. The other sheet in the file contains a citation for the paper by Koklu and Ozkan (2020) which, while useful for a bibliography, is irrelevant for data analysis.

```{r data_read, warning = FALSE}
# specify path
path <- paste0(getwd(), '/DryBeanDataset/Dry_Bean_Dataset.xlsx')
# read the first sheet (where the data is)
beans <- read_xlsx(path, sheet = 1)
```

## Exploratory Analysis

Before models could be trained and evaluated, the dataset itself was examined to reveal its format, number of potential predictors, and other information required for efficient data manipulation. Both the UCI Machine Learning Repository page and the Koklu and Ozkan (2020) paper were used as references for information not immediately obvious from the dataset alone.

### Overview

```{r data_overview}
# overview of the data's size and format
str(beans)
```
The dataset contains 13611 entries, each representing a scanned bean. There are 12 metrics describing the geometric features of each bean, plus four 'shape factors' which are derived from a previous machine learning article on rice grains (Koklu and Ozkan, 2020). All metrics except for `Class` (the outcome variable for this classification problem) are numeric, with `Class` being of class character.

A quick glance at the magnitudes of each numeric metric revealed large differences in scale; normalization will be needed for plotting and model fitting.

### Checking for NA Values

```{r na_check}
which(is.na(beans))
```
As described on the UCI repository webpage, the dataset indeed contains no NA entries.

### Distribution of Outcome Classes

```{r num_classes}
# checking the distribution of classes (bean types) in the dataset
beans %>% group_by(Class) %>%
  summarize(count = n())
```
**Table 1: Number of instances among the seven bean types (classes).**

The dataset has a rather uneven distribution of the 7 bean varieties. The most frequently found type is Dermason at 3546 entries, while the least common is Bombay at 522 entries. This uneven distribution must be accounted for during model fitting and accuracy assessments.

### Distribution of Potential Predictor Variables Among Classes

Before building models, the distribution of values among different bean varieties were visualized with boxplots for all 16 possible predictor variables. This was done to understand the levels of variability between bean varieties that were present in each potential predictor, and select the most informative predictors for models that perform best with lower-dimensional data.

As mentioned earlier, normalization was performed to produce all the plots featured within this report. While multiple formulae for normalization exist, the one used in this report is as follows:

$$X_{normalized} = \frac{(X - min)}{(max - min)}$$
Where $X$ is the value of a single instance within a metric, $max$ is the maximum value of the metric, and $min$ is the minimum value of the metric. This formula normalizes the values of each metric from 0 (minimum) to 1 (maximum).

*To conserve space, only the first instance in a series of very similar code chunks are displayed in the PDF file. A complete collection of code can be seen in the R Markdown and R files.*

#### Area

Koklu and Ozkan (2020) used a computer vision system (camera connected to a computer loaded with image-processing software) to scan each bean within the dataset. In their paper, the metric of Area was defined as the number of pixels within each scanned bean image, and denoted by $A$.

```{r Area_boxplot}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Area > (quantile(Area, 0.75)) + 1.5 * IQR(Area) | 
           Area < (quantile(Area, 0.25)) - 1.5 * IQR(Area))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Area), y = Area)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Areas')
```
**Figure 1: Normalized distribution of Areas among the seven bean varieties.**

The area IQR for Bombay is distinctively higher than those of all other varieties. Heavily overlapping IQRs exist in Cali and Barbunya, as well as Sira and Seker. Dermason, Seker, and Sira all have much narrower IQRs compared to the other varieties, despite having very close distribution ranges. Most outliers are on the upper ends of their respective distributions.

#### Perimeter

The metric of Perimeter (denoted by $P$) was defined as the length of each bean image's border (Koklu and Ozkan, 2020).

```{r Perimeter_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Perimeter > (quantile(Perimeter, 0.75)) + 1.5 * IQR(Perimeter) | 
           Perimeter < (quantile(Perimeter, 0.25)) - 1.5 * IQR(Perimeter))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Perimeter), y = Perimeter)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Perimeters')
```
**Figure 2: Normalized distribution of Perimeters among the seven bean varieties.**

Similarly to the Area metric, Bombay has a distinctively higher range of Perimeter lengths than the other varieties. The IQR for Cali exists entirely within that of Barbunya, while the IQRs for Dermason and Seker overlap. The IQRs of perimeter lengths for smaller bean varieties are less tight than seen in the Area graph, but also somewhat more distinct relative to the scale of the y-axis.

#### Major Axis Length

The Major Axis Length ($L$) was defined as the "distance between the ends of the longest
line that can be drawn from a bean" (Koklu and Ozkan, 2020); in other words, the length of each bean.

```{r MajorAxisLength_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(MajorAxisLength > (quantile(MajorAxisLength, 0.75)) + 1.5 * IQR(MajorAxisLength) | 
           MajorAxisLength < (quantile(MajorAxisLength, 0.25)) - 1.5 * IQR(MajorAxisLength))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, MajorAxisLength), y = MajorAxisLength)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Major Axis Lengths')
```
**Figure 3: Normalized distribution of Major Axis Lengths among the seven bean varieties.**

For Major Axis Length, Bombay once again has an IQR that was distinctively larger than all others. Barbunya/Horoz and Dermason/Seker have heavily overlapping IQRs, while Cali has a smaller IQR overlap with Horoz/Barbunya. Seker and Sira hav more outliers on the high end of their ranges, while Horoz has more on the low end.

#### Minor Axis Length

The Minor Axis Length ($l$) was described as the "longest line that can be drawn from the
bean while standing perpendicular to the main axis" (Koklu and Ozkan, 2020); it can be thought of as the width of each bean.

```{r MinorAxisLength_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(MinorAxisLength > (quantile(MinorAxisLength, 0.75)) + 1.5 * IQR(MinorAxisLength) | 
           MinorAxisLength < (quantile(MinorAxisLength, 0.25)) - 1.5 * IQR(MinorAxisLength))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, MinorAxisLength), y = MinorAxisLength)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Minor Axis Lengths')
```
**Figure 4: Normalized distribution of Minor Axis Lengths among the seven bean varieties.**

The distribution of Minor Axis Lengths are similar to that of Major Axis lengths, but with a few differences: here the heaviest overlap is between Cali and Barbunya, followed by Horoz/Sira and Sira/Seker. Also, Dermason has a partial IQR overlap with Horoz rather than a near-complete one with Seker. Most outliers are on the high ends of their respective groups. Cali, Seker, and Horoz have more outliers on th high ends of their respective groups, while Sira has more on the low end.

#### Aspect Ratio

The Aspect Ratio ($K$) was given in the original paper as the ratio between the Major and Minor Axis lengths (Koklu and Ozkan, 2020); in other words:

$$K = \frac{L}{l}$$

```{r AspectRation_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(AspectRation > (quantile(AspectRation, 0.75)) + 1.5 * IQR(AspectRation) | 
           AspectRation < (quantile(AspectRation, 0.25)) - 1.5 * IQR(AspectRation))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, AspectRation), y =AspectRation)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Aspect Ratios')
```
**Figure 4: Normalized distribution of Aspect Ratios among the seven bean varieties.**

The Aspect Ratio distributions differ from the previous potential predictors in that Bombay no longer has the most distinct distribution of values. Instead, Horoz and Cali have IQRs distinctively higher than others, while Seker has a lower IQR. The remaining varieties (Dermason, Barbunya, Sira, Bombay) all have heavily overlapping IQRs. Horoz has more outliers on the bottom end of its distribution, while Sira, Dermason, and Seker have more on their high ends.

#### Eccentricity

[Eccentricity](https://www.cuemath.com/geometry/eccentricity/) is a measure describing the shape of a[conic section](https://www.cuemath.com/geometry/conic-sections/). The eccentricity of each bean ($Ec$) was defined by Koklu and Ozkan (2020) as being the "eccentricity of an ellipse having the same [moments](https://byjus.com/jee/moment-of-inertia-of-ellipse/) as the [bean image] region".

The [eccentricity of an ellipse](https://www.cuemath.com/geometry/eccentricity-of-ellipse/) is in turn a ratio of two values: **the distance to any point on the ellipse from the [focus](https://www.cuemath.com/geometry/foci-of-ellipse/)**, and **the distance to that same point on the ellipse from the [directrix](https://www.cuemath.com/geometry/directrix-of-ellipse/)**.

```{r Eccentricity_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Eccentricity > (quantile(Eccentricity, 0.75)) + 1.5 * IQR(Eccentricity) | 
           Eccentricity < (quantile(Eccentricity, 0.25)) - 1.5 * IQR(Eccentricity))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Eccentricity), y = Eccentricity)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Eccentricities')
```
**Figure 5: Normalized distribution of Eccentricities among the seven bean varieties.**

The Eccentricity distributions are similar to that of Aspect Ratios, but with the two varieties having higher IQRs (Horoz, Cali) deviating from the others less while the one with lower IQR (Seker) deviating more. Most outliers are on the low ends of their respective ranges.

#### Convex Area

Koklu and Ozkan (2020) defined the Convex Area ($C$) of a bean as the "number of pixels in the smallest convex polygon
that can contain the area of a bean seed".

```{r ConvexArea_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ConvexArea > (quantile(ConvexArea, 0.75)) + 1.5 * IQR(ConvexArea) | 
           ConvexArea < (quantile(ConvexArea, 0.25)) - 1.5 * IQR(ConvexArea))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ConvexArea), y = ConvexArea)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Convex Areas')

```
**Figure 6: Normalized distribution of Convex Areas among the seven bean varieties.**

With the Covex Area Sizes, a distribution somewhat similar to the Area, Perimeter, and Axis Length distributions could be seen again. Bombay has a distinctively high IQR of Convex Areas, while Dermason has the lowest range. Cali and Barbunya have heavily overlapping IQRs, with less overlap existing among the other varieties. The narrowest IQRs belong to Seker and Sira. Outliers are biased towards the top end for Bombay, Cali, Barbunya, Sira, and Seker.

#### Equivalent Diameter

Koklu and Ozkan (2020) defined the Equivalent Diameter ($Ed$) as "the diameter of a circle having the same
area as a bean seed area".

```{r EquivDiameter_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(EquivDiameter > (quantile(EquivDiameter, 0.75)) + 1.5 * IQR(EquivDiameter) | 
           EquivDiameter < (quantile(EquivDiameter, 0.25)) - 1.5 * IQR(EquivDiameter))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, EquivDiameter), y = EquivDiameter)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Equivalent Diameters')
```
**Figure 7: Normalized distribution of Equivalent Diameters among the seven bean varieties.**

The distributions of Equivalent Diameters are fairly similar to that of Convex Areas. Once again, Cali and Barbunya have heavy overlaps, with Bombay having a distinctly higher IQR. Cali and Seker have more outliers on the top ends of their distributions, while Horoz has more on the bottom end.

#### Extent

The Extent ($Ex$) of a bean is a metric for size, being defined as "ratio of the pixels in the bounding box to the bean
area" (Koklu and Ozkan, 2020). What constituted a "bounding box" was not specified in the paper, but given its [mathematical definition](https://www.mathopenref.com/coordbounds.html) it was presumably a rectangular background containing all points within the bean image.

```{r Extent_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Extent > (quantile(Extent, 0.75)) + 1.5 * IQR(Extent) | 
           Extent < (quantile(Extent, 0.25)) - 1.5 * IQR(Extent))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Extent), y = Extent)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Extents')
```
**Figure 8: Normalized distribution of Extents among the seven bean varieties.**

The distributions of Extent Ratios exhibit heavy IQR overlaps between all bean varieties. Among the three groups with outliers (Cali, Seker, Bombay), all have more on the bottom ends of their respective distributions.

#### Solidity

Solidity ($S$) is another area-related metric, being the "ratio of the pixels in the
convex shell to those found in beans" (Koklu and Ozkan, 2020). Note that "convex shell" does not appear to be a commonly used methematical term; however, a similar term named "[convex hull](https://mathworld.wolfram.com/ConvexHull.html)" exists and the authors presumably meant the latter.

```{r Solidity_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Solidity > (quantile(Solidity, 0.75)) + 1.5 * IQR(Solidity) | 
           Solidity < (quantile(Solidity, 0.25)) - 1.5 * IQR(Solidity))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Solidity), y = Solidity)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Solidities')
```
**Figure 9: Normalized distribution of Solidities among the seven bean varieties.**

The distributions of Solidity values exhibit heavy IQR overlaps between nearly all bean varieties. Also, every bean variety has many outliers (values outside the boxplot whiskers) on the lower end of their respective distributions.

#### Roundness

Koklu and Ozkan (2020) gave the following formula for Roundness ($R$):

$$R = \frac{4\pi A}{P^2}$$
```{r roundness_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(roundness > (quantile(roundness, 0.75)) + 1.5 * IQR(roundness) | 
           roundness < (quantile(roundness, 0.25)) - 1.5 * IQR(roundness))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, roundness), y = roundness)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Roundnesses')
```
**Figure 10: Normalized distribution of Roundness values among the seven bean varieties.**

The ranking of median Roundness values are similar to that of that of Solidity, but with the lowest three (Cali, Barnunya, Horoz) switched. Less overlaps exist than in Solidity (except for heavy overlap Horoz and Barbunya), but many outliers outside the whiskers exist for all varieties, and are biased towards the low end of the range for all groups except Bombay.

#### Compactness

Compactness ($CO$) is another metric for bean roundness, and was given by Koklu and Ozkan (2020) as:

$$CO = \frac{Ed}{L}$$

```{r Compactness_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(Compactness > (quantile(Compactness, 0.75)) + 1.5 * IQR(Compactness) | 
           Compactness < (quantile(Compactness, 0.25)) - 1.5 * IQR(Compactness))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, Compactness), y = Compactness)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Compactness Values')
```
**Figure 11: Normalized distribution of Compactness values among the seven bean varieties.**

The distributions of Compactness values have heavy overlaps between Bombay, Sira, Barbunya, and Dermason. Seker has the highest range of Compactness, while Horoz has the lowest. All varieties have many outliers, mostly on the high ends of the distributions. Seker has most of its outliers on the low end of its range; the opposite was true for Bombay, Cali, and Horoz.

#### Shape Factor 1

In addition to the metrics described above, Koklu and Ozkan (2020) also made use of four "Shape Factors", from another machine learning article by [Pazoki et al. (2014).](https://www.scinapse.io/papers/345887372)

Shape Factor 1 ($SF1$) was defined as follows:

$$SF1 = \frac{L}{A}$$

```{r ShapeFactor1_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor1 > (quantile(ShapeFactor1, 0.75)) + 1.5 * IQR(ShapeFactor1) | 
           ShapeFactor1 < (quantile(ShapeFactor1, 0.25)) - 1.5 * IQR(ShapeFactor1))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor1), y = ShapeFactor1)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 1')
```
**Figure 12: Normalized distribution of SF1 values among the seven bean varieties.**

The distributions of Shape Factor 1 values feature heavy overlap between Barbunya/Cali, and relatively less overlap between Seker/Sira/Horoz. Dermason has the highest range of Shape Factor 1, and Bombay the lowest. All groups have more outliers on the top ends of their ranges except for Seker (more on the low end) and Bombay (two outliers on the low end only).

#### Shape Factor 2

Shape Factor 2 ($SF2$) was derived with the formula:

$$SF2 = \frac{l}{A}$$

```{r ShapeFactor2_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor2 > (quantile(ShapeFactor2, 0.75)) + 1.5 * IQR(ShapeFactor2) | 
           ShapeFactor2 < (quantile(ShapeFactor2, 0.25)) - 1.5 * IQR(ShapeFactor2))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor2), y = ShapeFactor2)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 2')
```
**Figure 13: Normalized distribution of SF2 values among the seven bean varieties.**

The distributions of Shape Factor 2 values have relatively less IQR overlap compared to most other variables examined, with the 4 highest Shape Factor 2 ranges (Seker, Dermason, Sira, Barbunya) all having non-overlapping IRRs. Horoz and Cali have a large IQR overlap, but neither has overlapping IQRs with Bombay. However, outliers on the high ends of the ranges are common. With the exception of Seker, all groups have more outliers on the top ends of their ranges.

#### Shape Factor 3

Shape Factor 3 ($SF3$) was derived from the formula:

$$SF3 = \frac{A}{({\frac{L}{2}})^2\pi}$$

```{r ShapeFactor3_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor3 > (quantile(ShapeFactor3, 0.75)) + 1.5 * IQR(ShapeFactor3) | 
           ShapeFactor3 < (quantile(ShapeFactor3, 0.25)) - 1.5 * IQR(ShapeFactor3))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor3), y = ShapeFactor3)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 3')
```
**Figure 14: Normalized distribution of SF3 values among the seven bean varieties.**

The distributions of Shape Factor 3 values have heavy overlaps in Bombay, Sira, Barbunya, and Dermason. Seker had a distinctively higher IQR than the other varieties but also many low outliers; the opposite was true for Horoz. Except for Seker, all groups have more outliers on the low ends of their ranges.

#### Shape Factor 4

Shape Factor 4 ($SF4$) was derived from the formula:

$$SF3 = \frac{A}{({\frac{L}{2}})(\frac{l}{2})\pi}$$

```{r ShapeFactor4_boxplot, echo = F}
# separately mark outliers
outliers <-
  beans %>%
  # do the following for each Class
  group_by(Class) %>%
  # filter for outliers; whiskers are 1.5 times IQR, while boxes go from Q1 to Q3
  filter(ShapeFactor4 > (quantile(ShapeFactor4, 0.75)) + 1.5 * IQR(ShapeFactor4) | 
           ShapeFactor4 < (quantile(ShapeFactor4, 0.25)) - 1.5 * IQR(ShapeFactor4))

beans %>%
  # plot with jittering outliers
  ggplot(aes(x = reorder(Class, ShapeFactor4), y = ShapeFactor4)) +
  # outliers plotted separately in geom_point()
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(data = outliers) + 
  xlab('Class') +
  ggtitle('Distribution of Shape Factor 4')
```
**Figure 15: Normalized distribution of SF4 values among the seven bean varieties.**

The distributions for Shape Factor 4 exhibit heavy overlaps between nearly all varieties, with the possible exception of Seker. In addition, every variety has many outliers on the low ends of their respective distributions. All groups have outliers on the low ends of their distributions; Horoz has an exceptionally long range of outlier values.

### Combining Predictors for Scatterplots

Some common classification models, such as KNN, do not perform efficiently with high-dimensional data due to a phenomenon known as the [Curse of Dimensionality](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/), where an increase in the amount of predictors (dimensionality) raises the possibility of overfitting the data in addition to making the data points sparser in space, requiring [less flexible models](https://rafalab.github.io/dsbook/examples-of-algorithms.html#the-curse-of-dimensionality) to maintain the same data coverage. As such, it would be constructive to select a pair of "compatible" predictor variables for those models. The goal is to have each variable contain inter-group (between bean varieties) variances that the other does not have sufficient amounts of. Also, the variables should be independent, lacking in [multicollinearity](https://www.theanalysisfactor.com/linear-regression-analysis-3-common-causes-of-multicollinearity-and-what-do-to-about-them/) of measurements. For instance, if one variable in the pair is already derived from bean lengths, the other variable should not also contain bean lengths in its formula. In addition, each of the two variables should have as much distance (or failing that, the least amount of overlap) between its group IQRs as possible, with the least amount of outliers and skew. Some variables (like Area and Convex Area) exist over a far larger numerical scale than others; this could be solved through normalizing all variables, making relative distances along the y-axis (as seen on the boxplots) more relevant than raw numerical differences.

To identify promising combinations, the variables were roughly grouped into six categories, reflecting broad features in their boxplots:

* Group 0 ("don't bother"): Variables whose boxplots display heavily overlapping IQRs and outliers that are visibly more numerous (compared to the other variables in this dataset) and/or heavily skewed in distributions. Extent, Solidity, and Shape Factor 4 were placed under this category.

* Group 1 ("Bombay high, Dermason low"): Variables where Bombay have the highest IQR (usually by a relatively large margin), and Dermason the lowest. Other bean varieties tend to have less separate or even overlapping IQRs. Area, Perimeter, Major Axis, Minor Axis, Convex Area, and Equivalent Diameter all fell under this category.

* Group 2 ("Horoz high, Seker low"): Variables where Horoz has the highest IQR, and Seker the lowest. Again, other groups tend to have less separate or overlapping IQRs. Aspect Ratio and Eccentricity fell under this cetegory.

* Group 3 ("Seker high, Horoz low"): The opposite of group 2.  Roundness and Compactness fell under this category. Note that as described in the original paper, Roundness and Compactness are both measures of how round a bean is, albeit calculated by different formulae. Both are also, by definition, the opposite metric of Aspect Ratio (which measures how elongated a bean is through the ratio between length/Major Axis Length and width/Minor Axis Width).

* Group 4 ("Dermason high, Bombay low"): The opposite of group 1, at least in terms of the defining feature (arbitrarily assigned for quick grouping). Shape Factor 1 was the sole variable assigned to this category.

* Group 5 ("Seker high, Bombay low"): Shape Factor 2 was the only variable assigned under this group.

To conserve memory, Group 0 variables were removed via the following code:

```{r trim_group0}
# trim off group 0 ("don't bother") variables, which will not be considered further
beans <-
  subset(beans, select = -c(Extent, Solidity, ShapeFactor4))
head(beans, 3)
```
Among group 1 variables, Perimeter and Equivalent Diameter appeared to be the most suitable choices, as they both have relatively less overlaps between the "middle groups" representing varieties other than Bombay and Dermason. Perimeter has large overlaps in Cali/Barbunya and Seker/Dermason, while Equivalent Diameter has large overlaps in Cali/Barbunya and Sira/Seker. Observing the boxplots, Aspect Ratio, Compactness, Shape Factor 2, and Shape Factor 3 were selected as the best complements due to having higher variances lacking in the group 1 choices, in addition to relatively less numerous and skewed outliers. Equivalent Diameter is incompatible with Compactness (Equivalent Diameter divided by Major Axis length), Shape Factor 2 (Minimum Axis length divided by Area), and Shape Factor 3 (which is derived from Area and Major Axis length). Meanwhile, Perimeter is compatible with Aspect Ratio, Compactness, Shape Factor 2, and Shape Factor 3. Included below are five scatterplots to further visualize the variables' ability to model bean varieties as distinct clusters of data points.

```{r Perim_AR}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         ar_tf = (AspectRation - min(AspectRation)) / (max(AspectRation) - min(AspectRation))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = ar_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Perimeter and Aspect Ratio Combinations on a Cartesian Plane')
```
**Figure 16: Scatterplot of Normalized Perimeter and Aspect Ratio values, color-coded by bean varieties.**

```{r Perim_Comp, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         comp_tf = (Compactness - min(Compactness)) / (max(Compactness) - min(Compactness))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = comp_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Compactness (Normalized)') +
  ggtitle('Normalized Perimeter and Compactness Combinations on a Cartesian Plane')
```
**Figure 17: Scatterplot of Normalized Perimeter and Compactness values, color-coded by bean varieties.**

```{r Perim_SF2, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         sf2_tf = (ShapeFactor2 - min(ShapeFactor2)) / (max(ShapeFactor2) - min(ShapeFactor2))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = sf2_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Shape Factor 2 (Normalized)') +
  ggtitle('Normalized Perimeter and Shape Factor 2 Combinations on a Cartesian Plane')
```
**Figure 18: Scatterplot of Normalized Perimeter and Shape Factor 2 values, color-coded by bean varieties.**

```{r Perim_SF3, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(perim_tf = (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter)),
         sf3_tf = (ShapeFactor3 - min(ShapeFactor3)) / (max(ShapeFactor3) - min(ShapeFactor3))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = perim_tf, y = sf3_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Perimeter (Normalized)') +
  ylab('Shape Factor 3 (Normalized)') +
  ggtitle('Normalized Perimeter and Shape Factor 3 Combinations on a Cartesian Plane')
```
**Figure 19: Scatterplot of Normalized Perimeter and Shape Factor 3 values, color-coded by bean varieties.**

```{r ED_AR, echo = F}
beans %>%
  # normalize variables: get the difference of each value from the minimum, then divide by the range
  # this process will also be used to train and test the algorithms
  mutate(ed_tf = (EquivDiameter - min(EquivDiameter)) / (max(EquivDiameter) - min(EquivDiameter)),
         ar_tf = (AspectRation - min(AspectRation)) / (max(AspectRation) - min(AspectRation))) %>%
  # plot with coloring by bean variety (Class)
  ggplot(aes(x = ed_tf, y = ar_tf, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Equivalent Diameter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Equivalent Diameter and Aspect Ratio Combinations on a Cartesian Plane')
```
**Figure 20: Scatterplot of Normalized Equivalent Diameter and Aspect Ratio values, color-coded by bean varieties.**

In all 5 scatterplots, adding another variable improved separation of the bean Class clusters that were poorly resolved in the group 1 variables (Barbunya/Cali, Sira/Seker, Seker/Dermason). However, large overlaps could still observed, especially in Dermason/Sira and Barbunya/Cali. Although the separation between Barbunya and Cali had improved from the near complete overlap in Perimeter and Equivalent Diameter boxplots, the scatterplots still revealed that a sizeable portion of the Barbunya cluster had been covered by the Cali cluster. Cluster visualization would be useful for further determination of which variable combination had the least amounts of overlap, but such a procedure would involve visualizing already tuned clustering algorithms and defeat the purpose of exploratory analysis. As a result, all five variable combinations were chosen for model building.

# Methods

With the insights gained from exploratory analyses and the original paper, the following steps were taken to prepare then analyze the data:

## Data Formatting

Before training any algorithms, the data was first split into two parts: a set containing all the potential predictors (the other 15 variables besides `Class`) named `beans_x`, and a set containing the bean varieties (`Class`) only named `beans_y`. This was done to comply with the argument format of `caret::train`, the function used to train all models in this project.

```{r x_y_split}
beans_x <- subset(beans, select = -c(Class))
beans_y <- subset(beans, select = c(Class))
head(beans_y, 3)
```
**Table 2: A very small sample of the beans_y dataset, as proof that the correct subset was selected.**

While `caret::train` is capable of processing predictor data in the dataframe format using the [formula](https://www.rdocumentation.org/packages/caret/versions/4.47/topics/train) `y ~ x1 + x2 + ...`, attempts to train algorithms using this input method resulted in repeated errors. As such, an alternative input method was chosen with the predictor entered in matrix format; `beans_x` was converted into matrix form to enable this decision.

```{r x_as_matrix}
beans_x <- as.matrix(beans_x)
head(beans_x, 3)
```

As previously done in the scatterplots (see Introduction section), the predictor variables must be normalized for algorithms such as KNN to work effectively. Since the variables vary greatly in scale, using them unaltered would lead to biased models that favor variables that exist over larger scales, [regardless of their actual importance](https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff) in relation to the outcome variable.

```{r x_normalization}
# for every column, apply the range formula onto each cell value:
# new value = (value - column min) / (column max - column min)
max_predictors <- apply(beans_x, 2, max) # column maxima
min_predictors <- apply(beans_x, 2, min) # column minima
colranges <- max_predictors - min_predictors # column ranges
beans_x <- sweep(beans_x, 2, STAT = min_predictors, FUN = '-') # subtract minima
beans_x <- sweep(beans_x, 2, STAT = colranges, FUN = '/') # divide by col ranges
# x is each matrix cell value, y is each entry in colranges
head(beans_x, 3)
```
```{r testing_matrix_norm_ED_AR, include = F}
# reconstitute the data table
as.data.frame(beans_x) %>%
  mutate(Class = beans_y$Class) %>%
 # plot with coloring by bean variety (Class)
  ggplot(aes(x = EquivDiameter, y = AspectRation, color = Class)) +
  geom_point(alpha = 0.6) +
  # using the viridis color scale for color-based accessibility
  scale_color_viridis(discrete = T) +
  xlab('Equivalent Diameter (Normalized)') +
  ylab('Aspect Ratio (Normalized)') +
  ggtitle('Normalized Equivalent Diameter and Aspect Ratio Combinations on a Cartesian Plane')
```

## Training and Validation

Simply using the entire dataset to train models incurs the risk of [overtraining](https://blog.roboflow.com/train-test-split/), where the model is well-fitted to the particular dataset used for training but generalizes poorly to new data of the same type. As such, the Dry Beans Dataset (both the predictor and outcome subsets) was split into two portions: a training set to train the models on, and a validation set simulating new data to evaluate the "real" performance of each model after training is complete.

Many different training:validation ratios have been proposed by machine learning resources online, with 90:10, 80:20, and 70:30 being especially common. Note that a trade-off exists between the sizes of training and validation sets due to the original dataset's finite size. While having a large pool of data for training is certainly beneficial as it reduces variance in the predicted outcomes, the validation dataset cannot get so small as to cause imprecise performance evaluation due to high variance in the [performance statistics](https://www.v7labs.com/blog/train-validation-test-set). 

With 13611 observations/beans, the Dry Bean Dataset is fairly large and allows for a somewhat more generous allocation of data to the validation set; [Baeldung CS](https://www.baeldung.com/cs/train-test-datasets-ratio) suggests a 70:30 split for small datasets containing less than 10000 observations, with a 80:20 split being applicable for most cases. As such, a training:validation split of 80:20 was selected for this project.

Note that in most online sources outside EdX, the comparison dataset used to gauge final model performance is called the "training set", and the comparison dataset used for tuning is the "vaidation set"; in short, the two terms are switched between EdX and outside sources. For the sake of consistency, this project uses the EdX naming conventions.

```{r data_split}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# creating the validation indeces
val_index <-
  createDataPartition(y = beans$Class,
                      times = 1,
                      p = 0.2,
                      list = F)
# split the data
# as the x-y split had been made earlier, the train-val split must be made for both x and y
beans_x_train <- beans_x[-val_index,]
beans_y_train <- beans_y[-val_index,]
beans_x_val <- beans_x[val_index,]
beans_y_val <- beans_y[val_index,]
```
Unlike with the previous project (MovieLens Rating Prediction), the models used in this project should not rely on the same titles being present in the training and validation (or testing) sets. As such, the `semi_join` function will not be used to ensure that everything in the validation set being present in the training set. However, we must still make sure that both sets have all 7 bean classes.

```{r val_set_dim}
dim(beans_x_val)
```
```{r val_set_classes}
as.data.frame(beans_y_val) %>%
  group_by(Class) %>%
  summarize(counts = n())
```
**Table 3: Distribution of bean varieties within the validation subset, split from the original Dry Beans Dataset.**

```{r train_set_rows}
dim(beans_x_train)
```
```{r train_set_classes}
as.data.frame(beans_y_train) %>%
  group_by(Class) %>%
  summarize(counts = n())
```
**Table 4: Distribution of bean varieties within the training subset, split from the original Dry Beans Dataset.**

With 2726 observations in the validation set and 10885 in the training set, the two portions add up to 13611 and no observations were lost during the split. Also, both sets have all 7 bean classes.

## Accuracy Metric

For a classification model, some [commonly used performance metrics](https://www.analyticsvidhya.com/blog/2021/07/metrics-to-evaluate-your-classification-model-to-take-the-right-decisions/) include accuracy, confusion matrix, and F1 score. Due to the class imbalance within the Dry Bean Dataset (see Introduction section), training models on accuracy could lead to misleading results where high accuracy scores are produced through predictions that favor more common classes within the dataset at the expense of rarer ones; such a situation is known as the accuracy paradox, and is caused by the model making predictions based on the imbalanced data distribution alone rather than any connections to the predictors. 

[Common ways to combat the accuracy paradox](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/) include taking larger samples, using a different performance metric, and resampling the dataset. For the choice of accuracy metric, the F1 score was chosen for this project for two reasons: it does not involve alterations of the data through resampling techniques, and it is robust against class imbalances.

The F1 score is the [harmonic mean](https://www.investopedia.com/terms/h/harmonicaverage.asp) of two metrics: precision (percentage of true positive predictions among all positive predictions) and recall (percentage of true positive predictions among actual positives).

The formula for precision is written as:

$$Precision = \frac{TP}{TP + FP}$$
Where $TP$ stands for True Positive predictions and $FP$ stands for False Positive predictions (actually negative, predicted as positive).

And the formula for recall is written as:

$$Recall = \frac{TP}{TP + FN}$$

Where $FN$ stands for False Negatives, data instances that are actually positive but incorrectly predicted as negative.

The F1 score formula then computes the harmonic mean as written below:

$$F1 = 2\frac{(Precision)(Recall)}{Precision + Recall}$$

For a binary classification problem, one outcome class could be arbitrarily defined as the positive and the other, the negative. However, since the Dry Beans Dataset contains more than two outcome classes, the simple positive/negative distinction must be expanded upon to derive [multiclass F1 scores](https://www.baeldung.com/cs/multi-class-f1-score). Multiclass F1 is computed by averaging the F1 score for each outcome category; when computing the F1 score of each class, the class in question is considered a "positive" and all other classes "negatives". Both macro F1 (unweighted average) and micro F1 (average F1 weighted by the size of each class) are used as accuracy metrics; as the goal of choosing F1 scores in this project is to avoid biases favoring larger classes, macro F1 was chosen for turning the models. For the Dry Beans Dataset, the macro F1 formula is thus:

$$mF1 = \frac{\Sigma_{class = 1}^7 F1_{class}}{7}$$
Where there are 7 classes, each denoted as $class$ in the formula.

Two functions, `macro_f1` and `f1`, were written; the former was designed to be a custom summary function under `caret::trainControl` and the later, a helper function for calculating individual F1 scores that together make up the macro F1.

```{r custom_multiclass_f1}
# this func should take same args as a function called defaultSummary()
# data = a dataframe with columns 'obs' and 'pred' (observed/actual and predicted)
macro_f1 <- function(data, lev = NULL, model = NULL) {
  mF1 <- mean(f1(data$pred, data$obs)) # arithmetic mean for macro F1
  names(mF1) <- 'mF1'# names the function output
  return(mF1)
}

f1 <- function(predicted, actual) {
  mat <- as.matrix(table(predicted, actual)) # should make a table where rows are pred, cols are actual/obs
  precision <- diag(mat) / rowSums(mat) # true pos / all predicted pos
  recall <- diag(mat) / colSums(mat) # true pos / all actual pos
  f1 <- ifelse(precision + recall == 0,
                      0,
                      2 * (precision * recall) / (precision + recall))
  return(f1)
}
```

As seen above, `macro_f1` (and all custom summary functions written for `caret::trainControl`) accepts a dataframe `data` with two columns: `obs` for observed outcomes, and `pred` for predicted outcomes. `f1` converts the dataframe into a [confusion matrix](https://machinelearningmastery.com/confusion-matrix-machine-learning/) tallying correct and incorrect predictions, where rows represent predicted outcomes and columns represent actual or observed outcomes. In such a matrix, a row sum is equal to the total predicted instances of a class, while a column sum represents the total actual instances of a class; the diagonal vector is then a series of correct predictions or True Positives.

```{r test_matrix, include = F}
test_table <- data.table(obs = c(1, 1, 2, 2, 2, 3, 4),
           pred = c(1, 1, 2, 2, 1, 3, 4))

test_table
```

```{r table_test, include = F}
as.matrix(table(test_table$pred, test_table$obs))
```
```{r f1_test, include = F}
f1(test_table$pred, test_table$obs) # okay so the f1 function returns something... and the right answers to boot!
```
```{r macro_f1_test, include = F}
macro_f1(test_table) # the macro_f1 func as a whole works
```

## Cross-Validation

Regardless of the algorithm used, [machine learning contains some randomness](https://machinelearningmastery.com/randomness-in-machine-learning/) on several levels. Sampling and splitting the dataset are random processes, as are the ordering of data instances fed into an algorithm and the iterative building of a predictive model using the algorithm. Setting random seeds before running each model imparts reproducibility, but the reality that each performance metric from a trained model is effectively a sampling statistic with variance attached remains.

One common way to reduce variance in machine learning is [k-folds cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/), a resampling technique that divides the training set into $k$ folds. For each fold, the remaining data (outside the fold) is used to build a model, with the model accuracy tested using the data within the fold. The performance metrics of all $k$ folds are then averaged to give a metric with lower variance; this is done for each hyperparameter (or combination of hyperparameters) tested during model tuning and training. [Common values](https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/) of $k$ include 3, 5, and 10; while higher values of $k$ are possible, 10 was chosen for this project as a compromise between reducing variance and avoiding overly long computational times. The parameters for a 10-fold cross validation, with each fold being 10 % of the training set, was specified using `caret::trainControl` as seen in the code chunk below.

Note that with 10-fold cross-validation, each fold would be 8 % (10 % of 80 %) of the full beans dataset.

```{r xval_method}
# 10-fold cross-validation
tenfold_xval <-
  trainControl(method = 'cv',
               number = 10,
               p = 0.9,
               summaryFunction = macro_f1) # custom macro f1 func in progress
```

## Model Choices

Many algorithms are available in the `caret` package for multiclass classification problems such as the one dealt with in this project. To demonstrate familiarity with the EdX program content, the following model types were chosen due to their detailed coverage in the machine learning course of the program:

### Logistic Regression

Logistic regression is a type of regression model used to find predictor-outcome correlations (and make predictions from them) in cases where the outcome/dependent variable is discrete rather than continuous. As its name suggests, logistic regression differs from linear regression in its [use of the logistic function]((https://machinelearningmastery.com/logistic-regression-for-machine-learning/), the basic form of which is written below:

$$f(x) = \frac{L}{1 + e^{-k(x-x_0)}}$$
This basic logistic function creates a sigmoid curve whose $f(x)$ value goes from approaching 0 to approaching $L$, with an inflection point at $x = x_0$. When applied to regression, the formula becomes this:

$$y = Pr(y = + | X = x) = \frac{e^{b_0 + b_1x}}{1 + e^{b_0 + b_1x}}$$
With $b_0$ being the intercept and $b_1$ being the coefficient for the variable $x$, like in linear regression. Unlike in linear regression, $y$ in logistic regression is not the outcome variable itself but the *probability* of $y$ turning out to be a given class (arbitrarily denoted as $+$ in the formula above) given a certain value of predictor $X$ (denoted as lower case $x$).

Then using this function:

$$g(y) = ln\frac{y}{1 - y}$$
The logistic regression function is made linear again, as shown below:

$$g(y) = g(Pr(y = + | X = x)) = b_0 + b_1$$
Where $g(y)$ is the natural log of the [odds ratio](https://psychscenehub.com/psychpedia/odds-ratio-2/), for the probability of $y$ being class $+$ given $X = x$.

For this project, five pairs of predictors (the ones visualized on the scatterplots from figures 16-20, in the Introduction section) will be used build models using logistic regression to circumvent the curse of dimensionality, with the highest-performing one being the representative for the algorithm. As mentioned in the introduction section, the five pairs were chosen due to their ability to complement each others' lack of variance among certain classes, and lack of any redundancy in their derivation which would result in multicollinearity.

Since the `glm` method in `caret::train` (the usual method for training logistic regression models) is suited for binary classification only, the method `multinom` was used instead. `multinom` allows for the tuning of decay, [a regularization term](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/) that penalizes variables with less contributions to the model. As only pairs of variables were selected and fed into the logistic regression algorithm, decay was set to 0.

### K-Nearest Neighbors (KNN)

[The KNN algorithm](https://learn.g2.com/k-nearest-neighbor) is a classification technique based on processes that are intuitive to understand. For each unclassified data point, the algorithm examines a set amount (the namesake $k$; not to be confused with the one from k-folds cross-validation) of nearest classified data points, before classifying the data point into the most represented class among the $k$ nearest neighbors; for instance, if $k = 10$ and Seker makes up 7 of the 10 neighbors, the new data point would be classified as Seker. [Euclidean distance](https://www.cuemath.com/euclidean-distance-formula/) is the default metric for neighbor "nearness" (and the one used in this project), although other distance metrics exist.

For this project, the method `knn` under `caret::train` was used to train all KNN-based models. Like in Logistic regression, the five pairs of predictors from figures 16-20 were used to avoid the curse of dimensionality. $k$ itself was the only hyperparameter to be tuned, and an arbitrary range of 1 to 100 was used in an attempt to capture the optimal $k$ over a large span of possible values.

### Decision Tree

[The Decision Tree algorithm](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html) works by randomly selecting a series of thresholds (for instance, "is normalized perimeter greater than 0.5?") to partition data by, selecting the split (called a node) that leads to maximum homogeneity (uniformity of outcome class) within each branch, and recursively split the data with more sub-nodes and branches until no more sub-nodes could be made (either because some constraint is met or the sub-node has only one data point left). 

For this project, the method `rpart` under `caret::train` was used. [The only allowed tuning parameter](https://topepo.github.io/caret/available-models.html) under the `caret` version of `rpart` is [Complexity Parameter](https://www.learnbymarketing.com/tutorials/rpart-decision-trees-in-r/) (CP), a measure of how much improvement (in terms of reducing incorrect classifications) must be achieved for a node to be made. An arbitrary CP range of 0.01 to 0.1 (in intervals of 0.01) was used in an attempt to capture the optimal CP over a large span; CP = 0 was avoided to avoid overfitting caused by unrestrained node splitting, while 0.01 was selected as the low end of the range since it is the [default CP value](https://www.rdocumentation.org/packages/rpart/versions/4.1.16/topics/rpart.control) for the standalone `rpart` function. Unlike with logistic regression and KNN, additional compatible variables were added on top of the best-performing pair from the previous two algorithms to examine the performance of higher-dimensional models.

### Random Forest

Since decision trees often fit the training data rather closely, they are naturally prone to overfitting. [A random forest](https://www.ibm.com/cloud/learn/random-forest) mitigates this issue by building multiple independent decision trees, each using a randomly selected subset of the predictor features. The trees are then averaged to provide predictions.

For this project, [the method](https://cran.r-project.org/web/packages/ranger/ranger.pdf) `ranger` under `caret::train` was used for a faster creation of random forests. The tuning hyperparameters consist of `mtry` (how many features to consider at each split; this project tried a range from 1 to all the predictors available), `splitrule` (the homogeneity metric to evaluate goodness of split; [Gini impurity](https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/) was chosen as the classification standard for `ranger`), and `min.node.size` (the minimal number of data points each node must have; an arbitrary range of 1-40 was chosen for wide coverage).

Despite being designed to run random forests faster than most other algorithms, `ranger` still consumed large amounts of time due to the 10-fold cross-validation and large amount of trees making the computational volume extremely large for the laptop used in this project. Due to practical reasons, a threshold of 2 hours (considered the maximum acceptable wait time, given the situation of this project's author at the time of its creation) was set for running all lines code up to and including the Random Forest. Only one combination (the best-performing one from the Decision Tree algorithm) of predictors were used, and the number of independent trees set to 250 (more trees were attempted, but resulted in run times exceeding 2 hours unless if the ranges of tuning hyperparameters were narrowed at the risk of possibly omitting optimal combinations).

# Results

## Model 1: Logistic Regression

```{r model1_Perimeter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,5)], # col 2 for perimeter, 5 for aspect ratio
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1a$results
```
**Table 4: Macro F1 score of Model 1a (Logistic Regression from Perimeter and Aspect Ratio).**

Logistic Regression using Perimeter and Aspect Ratio returned a macro F1 score of `0.8857739`, with a standard deviation of `0.01525563`.

```{r model1_Perimeter_Compactness}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,10)], # col 2 for perimeter, 10 for compactness
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1b$results
```
**Table 5: Macro F1 score of Model 1b (Logistic Regression from Perimeter and Compactness).**

Logistic Regression using Perimeter and Compactness returned a macro F1 score of `0.8903862	`, with a standard deviation of `0.01538134`.

```{r model1_Perimeter_ShapeFactor2}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1c$results
```
**Table 6: Macro F1 score of Model 1c (Logistic Regression from Perimeter and Shape Factor 2).**

Logistic Regression using Perimeter and Shape Factor 2 returned a macro F1 score of `0.8974943`, with a standard deviation of `0.009579192`.

```{r model1_Perimeter_ShapeFactor3}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1d <-
  train(y = beans_y_train$Class,
                x =  beans_x_train[,c(2,13)], # col 2 for perimeter, 13 for SF3
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1d$results
```
**Table 7: Macro F1 score of Model 1d (Logistic Regression from Perimeter and Shape Factor 3).**

Logistic Regression using Perimeter and Shape Factor 3 returned a macro F1 score of `0.890064`, with a standard deviation of `0.01603693`.

```{r model1_EquivDiameter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_1e <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(8,5)], # col 8 for equiv. diam., 5 for aspect ratio
        method = 'multinom', # multinomial log reg
        trControl = tenfold_xval, # use your 10-fold X-val 
        tuneGrid = data.frame(decay = 0), # no need for regularized variable weights
        metric = 'mF1') # my custom metric

model_1e$results
```
**Table 8: Macro F1 score of Model 1e (Logistic Regression from Equivalent Diameter and Aspect Ratio).**

Logistic Regression using Equivalent Diameter and Aspect Ratio returned a macro F1 score of `0.8820995`, with a standard deviation of `0.01410419`.

## Model 2: K-Nearest Neighbors (KNN)

```{r model2_Perimeter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,5)], # col 2 for perimeter, 5 for aspect ratio
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

# Warning in train.default(y = beans_y_train$Class, x = beans_x_train[, c(2,  :
#   The metric "mF1" was not in the result set.  will be used instead.
# -> using the names() func to name the output object of macro_f1 did the trick
ggplot(model_2a, highlight = T)
```
**Figure 21: Training macro F1 scores for Model 2a (Perimeter and Aspect Ratio) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 90` (within the diamond label) resulted in the highest macro F1 score, although all results from `k = 25` onwards are similarly high in macro F1.

```{r model_2a_besttune}
model_2a$bestTune # k = 90 is best for this one
model_2a$results[90,]
```
**Table 9: Macro F1 score of Model 2a (KNN from Perimeter and Aspect Ratio).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.8820995`, with a standard deviation of `0.01410419`.

```{r model2_Perimeter_Compactness}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,10)], # col 2 for perimeter, 10 for compactness
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2b, highlight = T)
```
**Figure 22: Training macro F1 scores for Model 2b (Perimeter and Compactness) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 86` (within the diamond label) resulted in the highest macro F1 score, although all results from `k = 25` onwards are similarly high in macro F1.

```{r model_2b_besttune}
model_2b$bestTune # k=86
model_2b$results[86,]
```
**Table 10: Macro F1 score of Model 2b (KNN from Perimeter and Compactness).**

KNN using Perimeter and Compactness returned a macro F1 score of `0.8908325`, with a standard deviation of `0.01170298`.

```{r model2_Perimeter_ShapeFactor2}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2c, highlight = T)
```
**Figure 23: Training macro F1 scores for Model 2c (Perimeter and Shape Factor 2) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 49` (within the diamond label) resulted in the highest macro F1 score, although all results from `k = 25` onwards are similarly high in macro F1. Unlike with the previous two KNN models, mF1 scores exhibited a slight and gradula decrease after `k = 49`.

```{r model_2c_besttune}
model_2c$bestTune # k = 49
model_2c$results[49,]
```
**Table 11: Macro F1 score of Model 2c (KNN from Perimeter and Shape Factor 2).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.9000603`, with a standard deviation of `0.009347538`.

```{r model2_Perimeter_ShapeFactor3}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2d <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,13)], # col 2 for perimeter, 13 for SF3
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2d, highlight = T)
```
**Figure 24: Training macro F1 scores for Model 2d (Perimeter and Shape Factor 3) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 79` (within the diamond label) resulted in the highest macro F1 score. Unlike with the previous KNN models, the plateauing of mF1 values occurred slightly later at a `k` value of 30-40.

```{r model_2d_besttune}
model_2d$bestTune # k=79
model_2d$results[79,]
```
**Table 12: Macro F1 score of Model 2d (KNN from Perimeter and Shape Factor 3).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.8907138`, with a standard deviation of `0.01205434`.

```{r model2_EquivDiameter_AspectRation}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

 # trying various k values (# nearest neighbors)
nearest_neighbors <- seq(1, 100, 1)

# using the y ~ x formula seems to keep throwing errors
model_2e <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(8,5)], # col 8 for equiv. diam., 5 for aspect ratio
        method = 'knn',
        tuneGrid = data.frame(k = nearest_neighbors),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

ggplot(model_2e, highlight = T)
```
**Figure 25: Training macro F1 scores for Model 2e (Equivalent Diameter and Aspect Ratio) with 10-fold cross validation, over a range of k from 0 to 100.**

`k = 74` (within the diamond label) resulted in the highest macro F1 score, although most mF1 values after `k = 25` are similarly high.

```{r model_2e_besttune}
model_2e$bestTune # k=74
model_2e$results[74,]
```
**Table 13: Macro F1 score of Model 2e (KNN from Equivalent Diameter and Aspect Ratio).**

KNN using Perimeter and Shape Factor 2 returned a macro F1 score of `0.884044`, with a standard deviation of `0.01043572`.

## Comparing Two-Variable Model Results

```{r models_1_2_table}
models_1_2 <-
  data.table(variables = c('Perimeter, Aspect Ratio',
                           'Perimeter, Compactness',
                           'Perimeter, Shape Factor 2',
                           'Perimeter, Shape Factor 3',
                           'Equivalent Diameter, Aspect Ratio'),
             logreg_mF1_unregularized = c(model_1a$results[,2],
                            model_1b$results[,2],
                            model_1c$results[,2],
                            model_1d$results[,2],
                            model_1e$results[,2]),
             knn_mF1 = c(model_2a$results[90,2],
                         model_2b$results[86,2],
                         model_2c$results[49,2],
                         model_2d$results[79,2],
                         model_2e$results[74,2]),
             k = c(90, 86, 49, 79, 74))
models_1_2
```
**Table 14: A summary of macro F1 values from all five of the predictor pairs tested for both Logistic Regression and KNN, plus the k value used to obtain the KNN results.**

KNN performed better than logistic regression in all 5 factor combinations, but not by much. In both models, Perimeter and Shape Factor 2 produced the best macro F1 scores (`0.9000603` for KNN, `0.8974943` for Logistic Regression). For the Decision Tree models, more variables will be included on top of those two to see if the macro F1 score can be improved further.

## Model 3: Decision Trees

To start off the Decision Tree models, the predictor combination of Perimeter and Shape Factor 2 (which performed optimal results with Logistic Regression and KNN) was used to tune and build a Decision Tree model.

```{r model3_Perimeter_ShapeFactor2}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.01, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3a <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12)], # col 2 for perimeter, 12 for SF2
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3a$results
```
**Table 15: Tuning results for Model 3a (Decision Tree from Perimeter and Shape Factor 2), with Complexity Parameters, macro F1 values, and standard deviations.**

For Model 3a (Decision Tree using Perimeter and Shape Factor 2), a CP of `0.01` produced the highest macro F1 (`0.8380341`) with a standard deviation of `0.016059859	`.


```{r model3_Perimeter_ShapeFactor2_MajorAxisLength}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.01, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3b <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,3)], # col 2 for perimeter, 12 for SF2, 3 for maj ax
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3b$results
```
```{r model3_Perimeter_ShapeFactor2_Eccentricity}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.01, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3c <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,6)], # col 2 for perimeter, 12 for SF2, 3 for Eccentricity
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3c$results
```
```{r model3_Perimeter_ShapeFactor2_MajorAxisLEngth_Eccentricity}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# Trying various Complexity Parameters
cps <- seq(0.01, 0.1, 0.01)

# using the y ~ x formula seems to keep throwing errors
model_3d <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,3,6)],
        method = 'rpart',
        tuneGrid = data.frame(cp = cps),
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_3d$results
```
```{r four_factor_tree}
plot(model_3d$finalModel)
text(model_3d$finalModel, cex = 0.5)
```


-multiple trees with 3-4 variables (if you can even find that many good ones)

-See which tree does best, then select that for RF

-Using all 4 appeared to be the best? Ask about plotting (plot too cluttered to be readable) while you do RF tomorrow

## Model 4: Random Forests

-Take the variables from the best DT, and run one (1) RF using ranger

```{r model4}
# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')

# RF tuning grid
forest_grid <-
  expand.grid(
    mtry = 1:4, # how many variables to use for splitting at each tree?
    # mtry = 2, # best mtry seems to be consistently 2 when adjusting min.node.size using 100 trees
    # let's go back to 1:4, see how it goes
    splitrule = 'gini', # for classification. Not changing this
    # min.node.size = 1:50 # for 100 trees; how many instances minimum in a node? Range rather arbitrary
    # min.node.size = 8:28 # for 500 trees; narrowed down from the 100-tree run
    # min.node.size = 1:50 # for 1K trees; readjusted from the 500-tree run
    # min.node.size = 10:30 # from the big 1K run; peak region is 10-30
    # min.node.size = 5:35 # expand a little to avoid cutting off max range for mtry = 1:4
    min.node.size = 1:40 # peak range could be a little more obvious
  )

# using the y ~ x formula seems to keep throwing errors
model_4 <-
  train(y = beans_y_train$Class,
        x =  beans_x_train[,c(2,12,3,6)],
        method = 'ranger',
        tuneGrid = forest_grid,
        # num.trees = 100, # arbitrary; will change with literature search
        # num.trees = 500, # progressively increasing...
        # num.trees = 1000, # too much
        # num.trees = 600, # try this one. If no better, try more or less...
        # num.trees = 550, # try to shrink back towards 500
        # num.trees = 500, # return to 500
        num.trees = 250, # 1K is too much; new goal is to find out how FEW trees you can get away with and still have similar performance qualities
        trControl = tenfold_xval, # use your 10-fold X-val 
        metric = 'mF1') # my custom metric

model_4$results
```

```{r model4_final_model}
model_4$bestTune
# with 100 trees per run, best tune is mtry = 2 (1-4) and min.node.size = 18 (1-50); mF1 = 0.9155445 +/- 0.009949667
# with 500 trees per run, best tune is mtry = 2 (no longer given option) and min.node.size = 12 (from 8 to 28); mF1 = 0.9155580 +/- 0.008680129
# with 1000 trees per run, best tune is mtry 2 (again, no choice) and min.node.size = 16 (2 to 20); mF1 = 0.9148272	+/- 0.009596302
# 1000 trees, opening min node to 1:50 but keeping mtry at 2: min = 21, mF1 = 0.9150073, mF1 = 0.010558968

# 600 trees, mtry 1:4, min nodes 10:30 -> best is mtry = 1, min node = 14; mF1 = 0.9153586 and mF1SD = 0.009217610
# 550 trees, mtry 1:4, min nodes 5:35 -> best is mtry = 1, min node = 18, mF1 = 0.9154987 +/- 0.009896242
# 500 trees, mtry 1:4, min nodes 1:40 -> best is mtry = 2, min node = 20, mF1 = 0.9154909	+/- 0.010352430
# 250 trees, mtry 1:4, min nodes 1:40 -> best is mtry = 2, min node = 16, mF1 = 0.9153936	 +/- 0.010256291; time is roughly 1 h 45 mins
# 200 trees, mtry 1:4, min nodes 1:40 -> best mtry = 2, min node = 24, mF1 = 0.9155035	+/- 0.009571362; time is roughly 1 h 25 mins
# there is a diminishing return thing going on for sure
# in the end, reducing # trees doesn't really make the model do better; it just makes the results more variable (plot more jagged; see below)
```
```{r model_4_plot}
plot(model_4) 
# when holding mtry at 2 and expanding range of nodes to 1:50, you see that the peak region is 10-30
# not so sure when mtry is 1:4 and num.trees is 600...
# note that with lower ntrees, the plot gets more jagged
# so how much do the numbers mean, really?

# eh, 200 trees isn't much more jagged than 500
```
## Validation Run
```{r validation_setup}
# convert beans_y_val$Class to class: Factor
beans_y_val$Class <- as.factor(beans_y_val$Class) # convert to factor class
levels(beans_y_val$Class) # checking that format is right
```

```{r validation_LR}
validation_preds_LR <- predict(model_1c$finalModel, beans_x_val)

confusionMatrix(validation_preds_LR, beans_y_val$Class)
```

```{r validation_KNN}
# KNN in R is not like the others, in that it fits and evaluates in one function
# Thus, a second training is done with the pre-tuned k value

# arbitrary seed for reproducibility
set.seed(1, sample.kind = 'Rounding')


validation_preds_KNN <- knn(train = beans_x_train[,c(2,12)],
                            cl = beans_y_train$Class,
                            test = beans_x_val[,c(2,12)],
                            k = 49) # KNN run on optimized k value tuned from caret::train

confusionMatrix(validation_preds_KNN, beans_y_val$Class)
```

```{r validation_2_factor_DT}
# DT preds take dataframes...
validation_preds_2DT <- predict(model_3a$finalModel, data.frame(beans_x_val), type = 'class')

confusionMatrix(validation_preds_2DT, beans_y_val$Class)
```

```{r validation_4_factor_DT}
validation_preds_4DT <- predict(model_3d$finalModel, data.frame(beans_x_val), type = 'class')

confusionMatrix(validation_preds_4DT, beans_y_val$Class)
```

```{r validation_RF}
validation_preds_RF <- predict(model_4$finalModel, beans_x_val)

confusionMatrix(validation_preds_RF$predictions, beans_y_val$Class)
```

-pick ultimate model, use it to predict validation set

-confusion matrix func comes with (balanced) accuracies; does not seem to have macro F1 and you don't know how to access the matrix values

-However, could always calc by hand and hide the code chunks if you really want mF1 for final results... The matrix is right there.

# Discussion

-TBD

# Conslusion

-TBD

# Citation

-TBD
